<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Eree&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://ereebay.me/"/>
  <updated>2020-03-03T14:14:32.929Z</updated>
  <id>https://ereebay.me/</id>
  
  <author>
    <name>Eree</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Decoupled Neural Interfaces using Synthetic Gradients</title>
    <link href="https://ereebay.me/posts/51469/"/>
    <id>https://ereebay.me/posts/51469/</id>
    <published>2020-02-18T03:04:59.000Z</published>
    <updated>2020-03-03T14:14:32.929Z</updated>
    
    <content type="html"><![CDATA[<h1 id="decoupled-neural-interfaces-using-synthetic-gradients">Decoupled Neural Interfaces using Synthetic Gradients</h1><h2 id="abstract">Abstract</h2><p>神经网络的训练通常需要计算图向前推到，然后再向后传播误差更新权重。因此从某种意义上来说，网络的所有层都被锁定了，因为他们必须等待网络其他部分向前推理并向后传播才能对其进行更新。在这项工作中，我们引入了网络图的未来计算模型，来通过对模块进行解耦，从而打破这个约束。这些模型仅使用局部信息即可预测子图的结果。尤其是，当我们专注于对误差梯度进行建模：通过使用建模的合成梯度来代替真实的反向传播误差梯度，从而使得子图解耦，并且可以独立且异步的更新子图，即我们实现了解耦的神经网络接口。我们展示了前馈模型的结果，其中的每一层都是异步训练的；RNN的结果，可以预测其中一层的未来的梯度，从而延长了RNN有效建模的时间；还有分层的RNN系统的结果，在不同的时标有刻度。最后，我们证明了，除了预测梯度之外，相同的框架还可以用于预测输入，从而导致模型在向前传播和向后传播都是解耦的，等于两个独立的网络，他们可以共同学习，从而将其组成一个单独的功能网络。</p><a id="more"></a><h2 id="introduction">Introduction</h2><p>有向神经网络中的每个层（或者模块）都可以视为一个计算步骤，转换其输入数据。这些模块通过有向边链接，从而建立了前馈图，该图定义了网络输入的数据流，通过每个模块，产生网络输出。对输出定义loss函数来产生误差并通过网络图反向传播回去以更新每个模块的权重。</p><p>这种机制导致了如下几个形式的锁定locking:</p><ol type="1"><li>Forward Locking 前向锁定：在执行前向推理图中的先前节点之前，没有模块能够处理其输入数据。</li><li>Update Locking 更新锁定：在所有相关模块在前向图执行完毕之前，无法更新任何模块。</li><li>Backwards Locking 反向锁定：在所有相关模块在前馈和反向模型执行完之前，没有模块可以被更新。比如BP算法。</li></ol><p>前向，更新，反向锁定限制了神经网络必须以顺序，同步的方式运行和更新。对于简单网络，看似良性，但是对于大型复杂不规则或异步时间尺度的多个环境中运行的网络系统就会有很大问题。</p><p>例如：分布式模型，其中模型的一部分是被许多下行的客户机共享和使用。这就意味着所有客户机必须完全执行并且将误差梯度传递会共享模型然后才能更新，这意味着这个系统训练速度由最慢的客户机决定。如果能够对当前网络的训练进行并行化，那么就可以极大地加快计算时间。</p><p>本项工作的目标是移除神经网络的更新锁定。这可以通过移除反向传播实现。为了更新模块i的权重<span class="math inline">\(\theta_{i}\)</span>，我们尽可能的近似了反向传播的函数：</p><p><span class="math display">\[\begin{aligned} \frac{\partial L}{\partial \theta_{i}} &amp;=f_{\text {Bprop }}\left(\left(h_{i}, x_{i}, y_{i}, \theta_{i}\right), \ldots\right) \frac{\partial h_{i}}{\partial \theta_{i}} \\ &amp; \simeq \hat{f}_{\text {Bprop }}\left(h_{i}\right) \frac{\partial h_{i}}{\partial \theta_{i}} \end{aligned}\]</span></p><p>其中h表示激活层，x是输入，y是监督（标签），L是总体损失。这使得其更新的依赖全部落在了h上，也就是模块i的局部信息。</p><p>该方法的前提是允许神经网络模块进行交互并且在没有更新锁定的情况下进行训练。在这里作者将传统的神经网络接口（神经网络中两个模块连接）替换为解耦神经接口（DNI）。简单来说，就是当一个网络层向另一个层传递激活值的时候，会有一个相关的模型，对该激活值产生一个预测的误差梯度。该预测的梯度仅仅是对该激活值的一个函数，而不依赖与其他事件状态或者损失。然后发送的那个网络层可以立即使用这个合成梯度来进行更新。通过移除更新和反向锁定，我们可以无需同步就能训练网络。我们还展示了初步的结果，将这一思想扩展并且也移除了前向锁定，从而使得网络的模块也可以在没有同步前向通过的情况下进行训练。当将其应用于RNN时，我们表明使用合成梯度可以使RNN建模的时间范围远大于BPTT的限制。我们还表明，使用合成梯度对两个在不同时间范围内的RNN进行解耦可以大大提高训练效果。</p><h2 id="decoupled-neural-interfaces">Decoupled Neural Interfaces</h2><p>首先描述high-level的通信协议，该协议用于允许异步学习agents进行通信。</p><p>正如图1所示， Sender A 发送信息（激活值）ha给Receiver B。简单来说可以理解为A是前面的layer，B是后一个layer。B有一个utility是MB用于处理信号ha，来预测反馈。误差信号：<span class="math inline">\(\hat{\delta}_{A}=M_{B}\left(h_{A}, s_{B}, c\right)\)</span> 其中，ha是信息（激活），B的状态SB，和一些其他的潜在信息c，例如标签或context。A可以立即利用该误差信号进行更新。B也可以及时完全评估真是的sigmaA，因此，B的utility模型可以被更新用于fit真是的utility，减少真实误差和合成误差的差异。</p><p>该协议允许A以A和B更新解耦的方式向B发送消息– A不必等待B评估真实效用就可以对其进行更新–并且A仍然可以学习发送消息到B。</p><p>我们可以将该协议用于网络的通信过程，从而产生所谓的去耦神经接口（DNI）。对于神经网络，反馈误差信号sigma_hat_A可以采用不同的形式，例如：梯度可以用作与反向传播一起使用的误差信号，目标信息可以用作与目标传播一起使用的误差信号，甚至可以用作结合到强化学习框架中的值。本文专注在通过反向传播和基于梯度更新的可微网络上。因此，专注于产生的误差梯度作为反馈sigma_hat_a，称为合成梯度。</p><p>Notation。我们将step为i的函数定义为fi，然后从stepi到stepj之间的函数的组成称为Fij。Layer i的损失定义为Li。</p><h3 id="synthetic-gradient-for-feed-forward-networks">Synthetic Gradient for Feed-Forward Networks</h3><p>考虑前馈网络的DNI形式，N个layers fi，每个接受输入hi-1，并产生输出hi=fi（hi-1），其中h0=x是输入数据。整个网络的前馈图可以表示为F1N。见图3（a）</p><figure><img src="http://cdn.ereebay.me/hexo/20200218134533.png" alt="20200218134533.png"><figcaption>20200218134533.png</figcaption></figure><p>定义网络输出的损失函数为L=LN。每个网络层fi有参数thetai，通过梯度更新的规则用来最小化L(hN）</p><p><span class="math display">\[\theta_{i} \leftarrow \theta_{i}-\alpha \delta_{i} \frac{\partial h_{i}}{\partial \theta_{i}} ; \quad \delta_{i}=\frac{\partial L}{\partial h_{i}}\]</span></p><p>α是学习率，对hi的求导是通过反向传播。对于sigmai的依赖意味着网络层i的更新必须在剩余部分更新后才行。即Fi+1N都已经执行了前馈和反馈阶段。因此，网络层i更新锁定在了Fi+1N。</p><p>为了移除更新锁定，采用前面提到的通信协议。网络层i发送信息hi给后一个网络层，其有通信模型Mi+1，来产生合成误差梯度sigma_hat_i = Mi+1(hi)。如图所示：</p><figure><img src="http://cdn.ereebay.me/hexo/20200218135141.png" alt="20200218135141.png"><figcaption>20200218135141.png</figcaption></figure><p>可以立即更新网络层i，以及F1i之间的其他网络层</p><p><span class="math display">\[\theta_{n} \leftarrow \theta_{n}-\alpha \hat{\delta}_{i} \frac{\partial h_{i}}{\partial \theta_{n}}, n \in\{1, \ldots, i\}\]</span></p><p>为了训练合成梯度模型Mi+1的参数，我们通过等待真实的误差梯度sigmai计算完成后，计算两者的mse误差。</p><p>此外，对于前馈网络，我们可以使用合成梯度作为通信反馈来解耦网络中的每个层。如图所示：</p><figure><img src="http://cdn.ereebay.me/hexo/20200218142500.png" alt="20200218142500.png"><figcaption>20200218142500.png</figcaption></figure><p>这个机制的完整执行过程：</p><figure><img src="http://cdn.ereebay.me/hexo/20200218150932.png" alt="20200218150932.png"><figcaption>20200218150932.png</figcaption></figure><p>在这种情况下，由于目标误差梯度sigmai是通过层i+1反向传播sigma_hat_i+1产生的。sigmai并不是真实的误差梯度，而是从后面的合成梯度模型获得的估计值。令人惊讶的是，这不会导致误差更加严重，而且即使在很多层中，也能保持稳定的学习，如后面实验所示。</p><p>此外，如果能够加上一些监督或者context c来计算合成梯度。</p><p><span class="math display">\[\hat{\delta}_{i} = {M_{i+1}\left(h_{i}, c\right) }\]</span></p><p>此过程允许在执行该层的前向传递后立即对其进行更新。这为以异步方式训练网络的子部分或层铺平了道路。</p><h2 id="experiment">Experiment</h2><h3 id="feed-forward-networks">Feed-Forward Networks</h3><p>将DNI应用于前馈网络，以允许异步或者零散的单层训练，这在分布式训练中有可能出现。</p><p>正如前面解释的，通过引入合成梯度使得网络层解耦，可以使得网络层之间彼此通信而不会收到更新锁定的影响。</p><p><strong>Asynchronous Updates</strong> 异步更新：为了演示通过DNI来实现的解耦层所带来的提升，我们在mnist的四层全连接网络上做了实验，其中，每层的反向传递和更新都是以概率pupdate随机顺序发生的。（即仅在前向传播了pudate的时间，网络层才会更新。）这将完全破坏反向传播，例如，第一层将仅接受概率pudate3的误差梯度更新，即使这样，系统也将被约束为同步。然而，通过DNI可以弥补每层之间的通信鸿沟，层更新的随机性不会影响下面的层，因为使用的是合成梯度。 我们使用了0到1之间均匀采样不同的pupdate的值。有标签和没标签的DNI如图7所示。</p><figure><img src="http://cdn.ereebay.me/hexo/20200218170640.png" alt="20200218170640.png"><figcaption>20200218170640.png</figcaption></figure><p>使用pupdate=0.2时，网络仍然可以训练得到2%的精度。令人难以置信的是，当DNI是以数据标签为条件时（如果以分布式方式进行训练，这是一个合理的假设），网络将以5%的更新概率完美进行训练，尽管速度变慢了。</p><h3 id="complete-unlock">Complete Unlock</h3><p>通过消除前向锁定完全使得前馈网络完全异步，这种情况下，每个网络层都会有一个合成的梯度模型，同时也有一个合成的输入模型 用于预测输入数据。如下图所示：</p><figure><img src="http://cdn.ereebay.me/hexo/20200218175611.png" alt="20200218175611.png"><figcaption>20200218175611.png</figcaption></figure><p>每个网络层都可以依靠合成梯度和合成的输入模型进行单独训练。下图是实验结果。</p><figure><img src="http://cdn.ereebay.me/hexo/20200218175713.png" alt="20200218175713.png"><figcaption>20200218175713.png</figcaption></figure><p>实验表明，在这种情况下，模型可以进行完全异步的独立训练，但是要达到2%的误差率精度，训练时间要更长一些。</p><h2 id="discussion-conclusion">Discussion &amp; Conclusion</h2><p>本文主要介绍了一种使用合成梯度的DNI方法，可以将网络层之间的通信进行解耦，让他们进行独立更新。也证明了该方法可以完全分离网络所有的层，从而使他们可以完全异步，非序，偶发的训练。</p><p>值得注意的是，尽管本文介绍并展示了DNI和合成梯度的有效性的经验依据，但是czarnecki等人的工作更深入研究了其理论理解，证实了收敛性。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;decoupled-neural-interfaces-using-synthetic-gradients&quot;&gt;Decoupled Neural Interfaces using Synthetic Gradients&lt;/h1&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;神经网络的训练通常需要计算图向前推到，然后再向后传播误差更新权重。因此从某种意义上来说，网络的所有层都被锁定了，因为他们必须等待网络其他部分向前推理并向后传播才能对其进行更新。在这项工作中，我们引入了网络图的未来计算模型，来通过对模块进行解耦，从而打破这个约束。这些模型仅使用局部信息即可预测子图的结果。尤其是，当我们专注于对误差梯度进行建模：通过使用建模的合成梯度来代替真实的反向传播误差梯度，从而使得子图解耦，并且可以独立且异步的更新子图，即我们实现了解耦的神经网络接口。我们展示了前馈模型的结果，其中的每一层都是异步训练的；RNN的结果，可以预测其中一层的未来的梯度，从而延长了RNN有效建模的时间；还有分层的RNN系统的结果，在不同的时标有刻度。最后，我们证明了，除了预测梯度之外，相同的框架还可以用于预测输入，从而导致模型在向前传播和向后传播都是解耦的，等于两个独立的网络，他们可以共同学习，从而将其组成一个单独的功能网络。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
      <category term="synthetic gradients" scheme="https://ereebay.me/tags/synthetic-gradients/"/>
    
  </entry>
  
  <entry>
    <title>Stagewise Knowledge Distillation</title>
    <link href="https://ereebay.me/posts/26959/"/>
    <id>https://ereebay.me/posts/26959/</id>
    <published>2020-02-17T05:15:34.000Z</published>
    <updated>2020-03-03T14:08:23.096Z</updated>
    
    <content type="html"><![CDATA[<h1 id="stagewise-knowledge-distillation">Stagewise Knowledge Distillation</h1><h2 id="abstract">Abstract</h2><p>大部分现代深度学习模型需要高运算力，但是对于嵌入式设备来说，缺少这种高运算能力。因此对于这类设备，能够减少运算并且保持性能的模型非常重要。知识蒸馏就是解决这类问题的方法之一。传统知识蒸馏方法是直接在一个阶段中将知识从老师中转换到学生。我们提出一种阶段性的训练方式，来提升知识的转换。这种方法甚至可以只用一部分训练教师模型的数据，并且不影响效果。这种方法可以补充其他模型压缩技术，甚至可以看做是通用的模型压缩技术。</p><a id="more"></a><h2 id="introduction">Introduction</h2><p>本文主要是在知识蒸馏上的模型压缩技术。因此先介绍一下模型压缩的分类。</p><p>模型压缩主要可以分为以下五类：</p><ol type="1"><li>Parameter Pruning and Sharing 参数修剪和共享：主要是为了减少网络参数中的冗余并且消除不必要的参数。</li><li>Low Rank Factorization techniques 低秩分解技术：主要是使用张量/矩阵分解来确定网络的有效参数。</li><li>Transferred/Compact Convolutional Filters 转换卷积过滤器：旨在使用专门设计的卷积过滤器以减少计算和存储空间。</li><li>knowledge Distillation 知识蒸馏： 旨在利用一个更大的与训练模型教师训练出一个小模型学生。</li><li>Quantization 量化：旨在减少每个权重的位数，同时保留网络性能。</li></ol><p>在这项工作中，主要专注于知识蒸馏的方法。理想情况下，教师应该能够将其所学的知识全部传授给学生，但是并非如此。此外老师所有的知识并不一定和学生有关，最理想的情况是，学生学习重要的环节，而忽略不重要的环节。本文主要采用resnet34作为教师模型，学生模型也采用类似resnet的模型，但是在存储结构和计算复杂度上要小得多。本文介绍了使用预训练的教师模型的多个特征图训练学生的方法。</p><p>本文主要采用一种新的方式进行训练，采用固定在某个层的教师模型的特征图对学生模型进行训练。针对每个特征图，以阶段的方式对学生模型进行训练，并且直接在数据集上训练最终的分类层，而无需老师。最终证明了，这种方法可以直接在teacher的一部分训练数据上学习。</p><h2 id="related-work">Related Work</h2><h2 id="methodology">Methodology</h2><p>本文主要采用的resnet网络，关于resnet的具体结构不赘述了主要就是：</p><ul><li>Basic Block</li><li>Downsample Block</li><li>ResNet18 or 34 type models</li></ul><h3 id="teacher-network">Teacher Network</h3><p>本文采用res34作为教师模型</p><h3 id="student-network">Student Network</h3><p>采用缩减版的res34作为学生模型，主要是减少basic block数量。</p><h3 id="dataset">Dataset</h3><p>数据集采用了三个：Imagenette Imagewoof 和 CIFAR10。前两个数据集是Imagenet的子数据集。第一个相对简单一些，后者相对难一点。本项工作的目的不是为了尽可能提升准确率，而是为了能够使得学生的准确率尽可能接近老师。</p><h3 id="proposed-training-method">Proposed Training Method</h3><p>在实验早期阶段中，我们训练多个学生模型的特征图以同时模仿教师模型中对应的特征图和标签。所以每对特征图的均方差误差将会被累计。此外，还会累计上交叉熵损失函数。因此总损失函数可以表示为：</p><p><span class="math display">\[\begin{aligned} L\left(y, \hat{y}, y_{c l s}, \mathrm{class}\right) &amp;=\frac{1}{M N} \sum_{i=1}^{N} \sum_{j=1}^{M}(y(i, j)-\hat{y}(i, j))^{2} \\ &amp;+\frac{1}{M C} \sum_{j=1}^{M} \sum_{k=1}^{C}\left\{-\log \left(\frac{\exp \left(y_{c l s}(j, \text { class })\right)}{\sum_{k=1}^{C} \exp \left(y_{c l s}(j, k)\right)}\right)\right\} \end{aligned}\]</span></p><p>N表示的是blocks的数量，y(i,j)是teacher模型，第i个block对应第j个输入的中间输出。同理yhat就是学生模型的。M是batchsize。ycls(j,k)是模型第k个类，第j个输入所对应的输出，C是类别的数量，而且类是每个特定输入所代表的正确的类。</p><p>这些早期实验表明了，在有和没有teacher模型的情况下训练的学生模型之间的非常小的提升。这些可以归因于多个特征图和标签必须同时被模仿，即优化算法的条件非常严格。给每个MSE损失和交叉熵损失分配权重也无济于事，因为训练过程仍然很严格。另一个原因可能是梯度消失和累计。为了减少这种训练的严格性，提出了分阶段训练方式。</p><p>我们对于学生模型采用分阶段的方式训练，也就是一个block一次。图像既是教师也是学生模型的输入，并且从两个模型中取出第一个块的输出。在输出之间采用mse误差，然后对学生模型进行反向传播。在训练第一个block 100个epoch之后，停止训练。在下一个步骤，再将输入传到teacher和学生，但取出第二个块的特征，然后按照和第一个阶段相同的步骤进行操作，即第二个块的输出之间的mse损失。然后反向传播100个epoch。对于所有的blocks都重复这个操作。在学生模型的末端，对分类器部分进行直接训练以从数据集中预测类别，即将图像传递到学生模型，并使用交叉熵损失对其进行训练以进行类别预测。在这个阶段，在此阶段，不使用教师模型，而学生模型的其余部分（除分类器部分之外的其他模型）都被冻结。使用图二可以很好理解这一点，第i阶段的训练损失函数可以表示为：</p><p><span class="math display">\[L_{i}(y, \hat{y})=\frac{1}{M} \sum_{j=1}^{M}(y(i, j)-\hat{y}(i, j))^{2}\]</span></p><p>分类器采用标准的交叉熵损失：</p><p><span class="math display">\[L_{c l s}\left(y_{c l s}, \mathrm{class}\right)=\frac{1}{M C} \sum_{j=1}^{M} \sum_{k=1}^{C}\left\{-\log \left(\frac{\exp \left(y_{c l s}(j, \mathrm{class})\right)}{\sum_{k=1}^{C} \exp \left(y_{c l s}(j, k)\right)}\right)\right\}\]</span></p><p>我们证明了阶段式训练有其自身的优点，主要优点是每次需要优化的参数数量有限。与上一次训练更多数量的参数相比，这种数量有限的参数可以是的训练时的严格程度放款。结果表明，阶段训练比一次性训练效果更好。</p><h4 id="less-data-apporach">Less Data Apporach</h4><p>像ImageNet这样的数据集是如此之大，以至于在优先的硬件上使用教师模型对学生模型进行分阶段训练将花费大量时间。因此，如果我们仅使用数据的一个子集执行分阶段训练，同时又能保持准确性，就变得很有用。因此，使用原始训练数据的1/4重复进行阶段式训练实验，注意原始训练数据是指在这些数据上训练教师模型的数据，剩余的3/4数据保留为测试集作为评估。</p><h2 id="results">Results</h2><p>图3和图4给出了同时训练，阶段训练，和部分数据的阶段训练的结果。</p><p>这些图表明，具有整个数据集的student模型几乎达到了和老师相同的准确性。但是在数据较少的情况下，在有老师和没有老师的情况下进行训练时，准确性存在巨大差距。以下各段讨论了这些结果背后的可能原因。应当指出，模型压缩的目的是减小教师和学生之间的差异，而不是得到更好的准确率。显然，如果使用更好的老师，学生的准确性将会提高，有时候，学生的准确性甚至超过了教师模型的准确性。</p><p>实验结果可以这样解释。由于teacher已经学习了完整的数据集，因此它已经学习到了用于对整个数据集进行分类的必要特征。当使用该teacher对student进行训练时候，即使使用小数据集来训练，也会将其“知识”传授给学生。使用较少的数据也可以通过在单个阶段训练中的必须的参数数量来证明。由于使用所提出的方法一次只训练一小部分网络，因此需要优化的参数数量比前面段落中提到的完整的网络少的多。结果表明，这种方法大大提高了准确性，在没有老师的情况下，在小型数据集上训练的学生网络的准确性要比使用提出的方法在同一数据集上训练的学生低得多。当然，主要优点是减少训练时间，这非常重要，因为在没有teacher的情况下，分阶段训练将花费N+1倍的时间进行训练。（因为每个阶段都单独训练了相同的epoch）。在这里n代表的是阶段。</p><p>图4显示了使用较少数据量的实验结果，可以看出，使用较少的数据量并且独立训练，学生的表现非常差。另一方面，如果较少的数据的教师模型来逐步训练学生，则预测的准确性会大大提高。这种能力应用于非常大的数据集将会非常有用。</p><p>同时训练的结果和没有老师的训练结果几乎接近。特别是，对于imagenet的两个子数据集，同时训练的结果略胜一筹，但是对于cifar10则略差。这重新证明了同步训练的条件过于严苛，而且对比没有teacher的训练情况下并没有明显优势。但是阶段训练的结果要比同时训练和没有teacher训练的结果都要好的多。由于在完整数据集上同时进行训练的结果并不乐观，因此在较少数据集上就不进行了。</p><h2 id="conclusion">Conclusion</h2><p>这项工作提出了一种新颖的方法将知识从一个网络转移到另一个网络。由于减少了在一个阶段优化的参数数量，因此与一次直接使用完整网络进行知识传输相比，该方法效果更好。这也使得学生网络的训练数据量可以少于教师。在类似于Imagenet这样的较大数据集训练时，将会非常有用。</p><p>此外，该方法非常灵活并且可以和其他模型压缩技术以及其他的模型一起使用。同时它也不局限于图像分类，还可以用于诸如对象检测图像分割等应用。可以看做是一种通用的压缩技术。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;stagewise-knowledge-distillation&quot;&gt;Stagewise Knowledge Distillation&lt;/h1&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;大部分现代深度学习模型需要高运算力，但是对于嵌入式设备来说，缺少这种高运算能力。因此对于这类设备，能够减少运算并且保持性能的模型非常重要。知识蒸馏就是解决这类问题的方法之一。传统知识蒸馏方法是直接在一个阶段中将知识从老师中转换到学生。我们提出一种阶段性的训练方式，来提升知识的转换。这种方法甚至可以只用一部分训练教师模型的数据，并且不影响效果。这种方法可以补充其他模型压缩技术，甚至可以看做是通用的模型压缩技术。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Federated Meta-Learning with Fast Convergence and Efficient Communication</title>
    <link href="https://ereebay.me/posts/35664/"/>
    <id>https://ereebay.me/posts/35664/</id>
    <published>2020-02-05T10:36:17.000Z</published>
    <updated>2020-03-03T14:09:06.104Z</updated>
    
    <content type="html"><![CDATA[<h1 id="federated-meta-learning-with-fast-convergence-and-efficient-communication">Federated Meta-Learning with Fast Convergence and Efficient Communication</h1><h2 id="abstract">Abstract</h2><p>本文提出了一个联邦元学习框架FedMeta，该结构共享参数化算法（meta learner）而不是以前的全局模型。本文在LEAF数据集和实际的数据集进行了评估，并且证明了FedMeta所需的通信成本降低了2.82-4.33倍，收敛速度更快，与FedAVG相比甚至提高了3.23~14.84个百分比。此外FedMeta保留了用户隐私，因为仅仅分享了参数化算法而没有数据。</p><a id="more"></a><h2 id="introduction">Introduction</h2><p>在联邦学习领域比较著名的FedAvg算法，可以灵活地使用SGD进行本地训练，在平衡计算和通信成本的情况下达到较高的精度。</p><p>在元学习领域，像MAML这种基于初始化的元学习算法非常擅长对于新的task的快速收敛以及拥有良好的泛化性。这使得其非常适合于non IID且高度个性化的去中心化数据。</p><p>受到这个启发，本文开发了一个与之前联合学习中工作大不相同的联合元学习框架。本文首先将元学习方法和联邦学习联系起来。在元学习中，通过元训练过程从大量的任务重缓慢学习参数化的算法（meta learner），在该过程中，算法会在每个task中快速训练一个特定的模型。</p><p>task通常由support set和query set组成，彼此不想交。在support set上训练特定任务的模型，在query set上进行测试。然后利用测试结果更新算法。在联邦元学习中，算法（meta learner）在server端维护，并分发到clients进行模型的训练。元训练的每个episode过程中，被采样到的一个batch的clients会接受到算法（meta learner）的参数然后进行模型训练。然后将query set上的测试结果上传到server端更新 meta learner。整体流程如图所示：</p><figure><img src="http://cdn.ereebay.me/hexo/20200218221422.png" alt="20200218221422.png"><figcaption>20200218221422.png</figcaption></figure><p><strong>Comparing federated meta-learning with federated learning</strong> FML和ML的比较。 联邦元学习类似于联邦学习，区别主要在于共享的不再是全局模型，而是算法参数（元模型参数）。但是，元学习在概念上和分布式模型训练不同，而且在共享元学习算法可以比共享模型更灵活应用。例如在图像分类中，n个类别的图像可能在clients之间不均匀地分布，其中每个client最多具有k个类别。对于联邦学习需要训练一个大型的n类分类器，以利用来自所有客户端的数据，然而其实k类分类器就足够了，因为他每次其实只为一个client做预测。对于联邦学习这种庞大的模型需要大量的通信成本，虽然可以只向client发送模型的一部分以更新相关参数，但这必须先了解client的私有数据才能决定。而另一方面，在元学习中，算法可以训练包含不同类别的任务。例如MAML可以通过对k类任务进行元训练来为K类分类器提供初始化，无论具体类别是什么。因此，在FML框架中，可以利用MAML对所有n个类别进行k类分类器初始化的元训练。也就是让一个k类分类器在n个类别的任务背景下进行初始化的元学习。这样FML的通信和计算成本都降低了。</p><p><strong>Contributions</strong> 本文贡献主要专注于联邦学习设置方面的算法设计，为此本文提出了一个新的框架并做了大量的实验。贡献点主要有三：1. 提出FedMeta框架，结合meta和fed，将maml算法和meta-sgd集成到federated learnin中。2. 在LEAF数据集上进行试验，与FedAvg进行比较，结果表明Fedmeta减少开销，同时精度更高。3. 将FedMeta应用于推荐任务，其中每个client都有高度personalized的记录，通过实验证明，与独立或联邦学习方法系那个比，元学习算法可以实现更高准确性。</p><h2 id="federated-meta-learning">Federated Meta-Learning</h2><h3 id="the-meta-learning-approach">The Meta-Learning Approach</h3><p>元学习的目的是meta-train一个算法（meta learner）A，能够快速的训练一个模型。也就是能够得到一个能够快速收敛的初始模型。算法<span class="math inline">\(A_{\varphi}\)</span>是参数化的模型，其参数在元训练过程中，通过一系列的tasks进行更新。 元训练过程中，一个task T包含了一系列的 support set和query set，每个set都有对应的标签和数据。算法（meta learner）A会在support set上对模型f进行训练，并且输出参数<span class="math inline">\(\theta_T\)</span>，这成为inner update。然后模型<span class="math inline">\(f_{\theta_T}\)</span>会在query set上进行评估，然后会计算test loss<span class="math inline">\(\mathcal{L}_{D_{Q}^{T}}\left(\theta_{T}\right)\)</span> 来反映算法A的训练能力。最后A通过最小化test loss来进行更新，这个步骤称为outer update。这里要注意的是query set和support set是disjoint的，以最大化A的泛化能力。元训练阶段中在每个episode会从一个meta training set中样一个batch的tasks。因此算法A的优化目标可以表示为：</p><p><span class="math display">\[\min _{\varphi} \mathbb{E}_{T \sim \mathcal{T}}\left[\mathcal{L}_{D_{Q}^{T}}\left(\theta_{T}\right)\right]=\min _{\varphi} \mathbb{E}_{T \sim \mathcal{T}}\left[\mathcal{L}_{D_{Q}^{T}}\left(\mathcal{A}_{\varphi}\left(D_{S}^{T}\right)\right)\right]\]</span></p><p>MAML是具有代表性的元学习算法。对于MAML的算法A就是用于产生模型的初始状态。具体地说，就是对于每个task T，算法使得<span class="math inline">\(\alpha = \theta\)</span>，使得算法的参数和模型f的参数相等。然后模型f的参数在support set上训练，根据损失函数进行更新：<span class="math inline">\(\mathcal{L}_{D_{S}^{T}}(\theta):=\frac{1}{\left|D_{S}^{T}\right|} \sum_{(x, y) \in D_{S}^{T}} \ell\left(f_{\theta}(x), y\right)\)</span>。最后把模型参数在query set上进行测试，然后计算测试的损失：<span class="math inline">\(\mathcal{L}_{D_{Q}^{T}}\left(\theta_{T}\right):=\frac{1}{\left|D_{Q}^{T}\right|} \sum_{\left(x^{\prime}, y^{\prime}\right) \in D_{Q}^{T}} \ell\left(f_{\theta_{T}}\left(x^{\prime}\right), y^{\prime}\right)\)</span></p><p>公式1可以简写为：</p><p><span class="math display">\[\min _{\theta} \mathbb{E}_{T \sim \mathcal{T}}\left[\mathcal{L}_{D_{Q}^{T}}\left(\theta-\alpha \nabla \mathcal{L}_{D_{S}^{T}}(\theta)\right)\right]\]</span></p><p>基于MAML，Meta-SGD进一步学习初始参数和内循环学习率在同一时刻。测试损失可以看做是模型参数和学习率的函数，两者都可以在外循环中采用SGD算法求梯度来被更新。此外，学习率和模型参数是相同维度的向量，是的学习率在坐标方向上和模型参数向量相对应。</p><p>因此优化条件可以重写为</p><p><span class="math display">\[\min _{\theta, \alpha} \mathbb{E}_{T \sim \mathcal{T}}\left[\mathcal{L}_{D_{Q}^{T}}\left(\theta-\alpha \circ \nabla \mathcal{L}_{D_{S}^{T}}(\theta)\right)\right]\]</span></p><h3 id="the-federated-meta-learning-framework">The Federated Meta-Learning Framework</h3><p>FML的 目标是能够协作式的通过分布在各个clients的数据meta train一个算法。以MAML为例，就是希望那个利用所有clients的数据，来完成一个模型的初始化。MAML包括两个层次的优化过程：内循环利用初始化参数训练特定任务模型，外循环根据测试损失更新初始化参数。在联邦学习的环境中，每个client u会从服务器接收初始化参数 <span class="math inline">\(\theta\)</span>，然后根据设备上的support set数据进行模型的训练，并传输test loss到server。server主要维护初始化参数，并根据clients的test loss对其进行更新。</p><p>在这个过程中传输的信息包括了：模型初始化的参数（从server 到clients）和test loss（从clients到server）。对于Meta-SGD算法，向量<span class="math inline">\(\alpha\)</span>也是传输的部分，用于内循环模型训练的学习率参数。</p><figure><img src="http://cdn.ereebay.me/hexo/20200219111333.png" alt="20200219111333.png"><figcaption>20200219111333.png</figcaption></figure><p>算法1描述了采用MAML和MetaSGD的FedMeta的过程，其中communication round表示的是episode。算法在AlgorithmUpdate步骤中被维护。在更新的每轮，server会收集MAML或者metasgd在采样的clients上训练的得到的test loss。初始化参数<span class="math inline">\(\theta\)</span>在client的训练集训练后更新，然后使用更新后的参数在测试机上测试。然后在meta trian过程后将模型部署在client上。</p><h2 id="experiments">Experiments</h2><ol type="1"><li>在LEAF数据集上实验，证明了其收敛性块，准确性高。</li><li>在实际环境下的推荐任务中进行实验，可以保证在较小的规模依然保持较强的适应力。</li></ol><h3 id="evaluation-scheme">Evaluation Scheme</h3><p>所有实验中80%的clients作为训练clients，10%作为验证clients剩余的作为测试clients。每个clients的本地数据会分为support set和query set。同时本文改变每个client的p部分的数据作为support set来研究FedMeta如何有效适应数据量有限的新client。后面简称为psupport。</p><figure><img src="http://cdn.ereebay.me/hexo/20200219114337.png" alt="20200219114337.png"><figcaption>20200219114337.png</figcaption></figure><p>作者做了FedAVG，FedAVG的meta版本以及两种FedMeta的实验。FedAVG的meta版本是利用测试clients的support set 来对从server接收到的初始化模型进行finetune，然后测试。而训练阶段，两者都采用训练clients上所有的数据。</p><p>至于FML，测试了三种MAML，FOMAML，Meta-SGD，都是模型无关的方法，而且容易实现。</p><h3 id="leaf-datasets">LEAF Datasets</h3><p><strong>Accuracy and Convergence Comparsion</strong> 考虑到边际设备的运算能力的限制，每个client的local epoch设置为1。</p><figure><img src="http://cdn.ereebay.me/hexo/20200219125820.png" alt="20200219125820.png"><figcaption>20200219125820.png</figcaption></figure><p>如上图所示，FedMeta框架所有方法都更快更稳定的收敛了，并且得到了明显的提升。MAML和Meta-SGD的收敛速度很准确率在前两个数据集都达到了相似的收敛速度和准确度。在Sent140上meta-sgd更好一些。</p><p>表格展示了四种方法数轮通信后的准确度。首先可以看出，FedAvg表现远远差于FedMeta，尤其是图像分类的任务。MAML和Meta-SGD的准确度最高，增加了3.23-14.84的准确度。同时也发现，fedavg（meta）在大多数情况比fedavg准确率更高。然而，有两个例外，就是当support fraction为20%的时候。出乎意料的是FedAvg（Meta）还有在精准度上轻微的下降。这可能是因为在对少量数据进行微调后，该模型和全局最优值存在过度偏差。其次，当我们增加support集的概率p，几乎在所有情况下FedAvg（Meta）和FedMeta的精度都会提高。但是FedAvg（meta）的增长率大于FedMeta。说明FedMeta泛化能力更强，更有效适应数据量有限的client。</p><p><strong>Fairness Comparison</strong> 本文同时还比较了多次实验，利用平均最终的准确度分布，来研究FedAvg和FedMeta。从前面的图的最后一行，展示了不同方法的核密度估计。对于FEMNIST，MAML和Meta-SGD不仅导致较高的均值，而且方差更低。对于Shakespeare数据集，FedMeta方差虽然更大，但是均值也更大。对于Sent140，准确度的分布都差不多。然而可以看出，MAML和Meta-SGD有更多接近100%的clients。总之，对于图像分类任务，FedMeta的准确度分布更为稳定集中。</p><h2 id="conclusion">Conclusion</h2><p>FedMeta框架，在准确性 收敛速度和通信成本都比原本的FedAvg要更好。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;federated-meta-learning-with-fast-convergence-and-efficient-communication&quot;&gt;Federated Meta-Learning with Fast Convergence and Efficient Communication&lt;/h1&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;本文提出了一个联邦元学习框架FedMeta，该结构共享参数化算法（meta learner）而不是以前的全局模型。本文在LEAF数据集和实际的数据集进行了评估，并且证明了FedMeta所需的通信成本降低了2.82-4.33倍，收敛速度更快，与FedAVG相比甚至提高了3.23~14.84个百分比。此外FedMeta保留了用户隐私，因为仅仅分享了参数化算法而没有数据。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>CS330 lecture 1&amp;2 学习笔记【未完】</title>
    <link href="https://ereebay.me/posts/28331/"/>
    <id>https://ereebay.me/posts/28331/</id>
    <published>2020-01-10T03:31:05.000Z</published>
    <updated>2020-03-03T14:14:20.352Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs330-lecture-12-notes">CS330 lecture 1&amp;2 notes</h1><h2 id="informal-problem-definitions">Informal Problem Definitions</h2><ul><li>The multi-task learning problem: Learn all of the tasks more quickly or more proficiently than learning them independently.(更快更专业学习所有任务)</li><li>The meta-learning problem: Given data/experience on previous tasks, learn a new task more quickly and/or more proficiently.（依照先前任务的经验，更快更好的学习新任务）</li></ul><a id="more"></a><h2 id="multi-task-learning-basics">Multi-Task Learning Basics</h2><p>传统单任务学习 Single-task learning:</p><p><span class="math display">\[\begin{array}{l}{\mathscr{D}=\left\{(\mathbf{x}, \mathbf{y})_{k}\right\}} \\ {\min _{\theta} \mathscr{L}(\theta, \mathscr{D})}\end{array}\]</span></p><p>损失函数 Typical loss: negative log likelihood</p><p><span class="math display">\[\mathscr{L}(\theta, \mathscr{D})=-\mathbb{E}_{(x, y) \sim \mathscr{D}}\left[\log f_{\theta}(\mathbf{y} | \mathbf{x})\right]\]</span></p><h3 id="whats-a-task">What’s a task?</h3><p>A task: <span class="math inline">\(\mathscr{T}_{i} \triangleq\left\{p_{i}(\mathbf{x}), p_{i}(\mathbf{y} | \mathbf{x}), \mathscr{L}_{i}\right\}\)</span></p><p>data generating distributions</p><p>这里将任务定义成，数据样本的分布，数据标签的分布和损失函数</p><p>Corresponding datsets:<span class="math inline">\(\mathscr{D}_{i}^{tr}\)</span> 训练集<span class="math inline">\(\mathscr{D}_{i}^{t s t}\)</span>测试集 通常用<span class="math inline">\(\mathscr{D}_{i}\)</span>表示训练集</p><p>Multi-task classification：<span class="math inline">\(\mathscr{L}_{i}\)</span> same across all tasks 例如：每种语言的手写识别中，损失函数的形式可能相同</p><p>Multi-label learning:<span class="math inline">\(\mathscr{L}_{i}, {p}_{i}(x)\)</span> same across all tasks 例如：CelebA 多标签识别任务中，样本和损失函数都是相同的</p><p>在以下任务中损失函数可能会发生不同的变化：</p><ul><li>mixed discrete， continuous labels across tasks（混了了离散和连续的标签的任务）</li><li>care more about one task than another（任务之间权重不同？）</li></ul><h3 id="conditioning-on-the-task">Conditioning on the task</h3><p>对于multi task learning的问题需要引入一个task descriptor作为描述任务的变量，那么如何设计这个变量。</p><p>假设<span class="math inline">\({z}_{i}\)</span>是task index，那么最直接的办法就是multiplicative gating，也就使得这个多任务学习中的每个任务是独立的用单独网络训练而不分享参数。</p><p>那么另一种极端就是直接concat<span class="math inline">\(z_i\)</span>,那样所有参数都会被共享，除了输入<span class="math inline">\(z_i\)</span>之后的参数。</p><p>那么另一种思路就是吧<span class="math inline">\(\theta\)</span>分为shared parameters <span class="math inline">\(\theta^{sh}\)</span>和task-specific parametes <span class="math inline">\(\theta^i\)</span>也就是共享参数和不共享参数</p><p>那么优化目标就变成了</p><p><span class="math display">\[\min _{\theta^{s h}, \theta^{1}, \ldots, \theta^{T}} \sum_{i=1}^{T} \mathscr{L}_{i}\left(\left\{\theta^{s h}, \theta^{i}\right\}, \mathscr{D}_{i}\right)\]</span></p><p>问题就转化成了如何选择和什么时候共享参数</p><h4 id="common-choices">Common Choices</h4><p>常用的方法主要有拼接和加法，看图一目了然 1. Concatenation-based conditioning（拼接)</p><figure><img src="http://cdn.ereebay.me/hexo/cs330-1-1.png" alt="cs330-1-1.png"><figcaption>cs330-1-1.png</figcaption></figure><ol start="2" type="1"><li>Additive conditioning（加法）</li></ol><figure><img src="http://cdn.ereebay.me/hexo/cs330-1-2.png" alt="cs330-1-2.png"><figcaption>cs330-1-2.png</figcaption></figure><p>实际上两者是等同的</p><figure><img src="http://cdn.ereebay.me/hexo/cs330-1-3.png" alt="cs330-1-3.png"><figcaption>cs330-1-3.png</figcaption></figure><ol start="3" type="1"><li>Multi-head architecture</li></ol><figure><img src="http://cdn.ereebay.me/hexo/cs330-1-4.png" alt="cs330-1-4.png"><figcaption>cs330-1-4.png</figcaption></figure><ol start="4" type="1"><li>Multiplicative conditioning</li></ol><figure><img src="http://cdn.ereebay.me/hexo/cs330-1-5.png" alt="cs330-1-5.png"><figcaption>cs330-1-5.png</figcaption></figure><p>乘法的方法可以带来 - 更强的表达能力 - 对于回归任务，有multiplication gating - 能够对于独立的网络和heads进行更好的泛化</p><h4 id="complex-choices">Complex Choices</h4><p>还有许多其他复杂的方式</p><figure><img src="http://cdn.ereebay.me/hexo/cs330-1-6.png" alt="cs330-1-6.png"><figcaption>cs330-1-6.png</figcaption></figure><p>但是设计的灵感来源问题就和神经网络的参数选择一样： - 不同问题之间是独立的 - 对于特定的问题，主要依靠设计者的直觉和背景知识 - 目前解决的方法更像是艺术而非科学</p><h3 id="optimizing-the-objective">Optimizing the objective</h3><p>Objective:<span class="math inline">\(\min _{\theta} \sum_{i=1}^{T} \mathscr{L}_{i}\left(\theta, \mathscr{D}_{i}\right)\)</span></p><p>一般的流程：</p><ol type="1"><li>从tasks中采样一个minibatch<span class="math inline">\(\mathscr{B} \sim\left\{\mathscr{T}_{i}\right\}\)</span></li><li>从每个task中采样一个minibatch的数据样本<span class="math inline">\(\mathscr{D}_{i}^{b} \sim \mathscr{D}_{i}\)</span></li><li>在每个minibatch-task上计算loss：<span class="math inline">\(\hat{\mathscr{L}}(\theta, \mathscr{B})=\sum_{\mathcal{T}_{k} \in \mathscr{B}} \mathscr{L}_{k}\left(\theta, \mathscr{D}_{k}^{b}\right)\)</span></li><li>反向传播计算梯度<span class="math inline">\(\nabla_{\theta} \hat{\mathscr{L}}\)</span></li><li>采用你喜欢的优化器更新梯度</li></ol><p>Note：这样可以保证无论数据量如何，任务都可以被均匀采样 Tip:对于回归任务，确保任务标签是相同的scale</p><h3 id="challenge">Challenge</h3><ol type="1"><li>Negative transfer</li></ol><p>在cifar-100上进行多任务训练，不如独立训练。</p><p>可能的原因： - 来自优化方面的挑战 - 不同任务之间的相互干扰 - 不同任务的学习速率不同 - 受限的表达能力 - 多任务网络体积较大</p><p>解决方法：</p><p>share less across task（软参数分享） - allows for more luid degrees of parameter sharing 好处 - yet another set of design decisions/hyperparameters 坏处</p><p><span class="math display">\[\min _{\theta^{sh}, \theta^{1}, \ldots, \theta^{T}} \sum_{i=1}^{T} \mathscr{L}_{i}\left(\left\{\theta^{s h}, \theta^{i}\right\}, \mathscr{D}_{i}\right)+\sum_{t=1}^{T}\left\|\theta^{t}-\theta^{t&#39;}\right\|\]</span></p><p>后面的部分就是软参数分享，就是将第二个task的参数和前一个的差作为优化项，相当于让每个task的参数尽可能相似，也就是分享了参数。</p><ol start="2" type="1"><li>Overfitting</li></ol><p>过拟合原因一般是由于，参数共享的不够多，解决方案就是参数分享的更多一些。可以理解为，分享的不够，使得每个task都过拟合了，类似于独立训练了。</p><h2 id="meta-learning-basics">Meta-Learning Basics</h2><p>元学习的两种视角 - Mechanistic view （机制视角） - 可以读取整个数据集并且对新的数据进行预测的深度神经网络 - 利用元数据集对网络进行训练，并且这种元数据集针对不同的任务包含了不同的数据集 - 这种视角可以容易实现一个元学习算法。 - Probabilistic view（概率视角） - 从一系列元学习任务重提取先验知识 - 使用少量的数据和先验信息推测出相对有效的后验参数 - 这种视角可以更好地理解元学习算法</p><h3 id="problem-definitions">Problem definitions</h3><p>首先回顾一下监督学习： <img src="http://cdn.ereebay.me/hexo/cs330-1-7.png" alt="cs330-1-7.png"></p><p>存在的问题： - 需要大量的标记数据 - 目前的某些任务数据标签很有限</p><p>To be continued</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;cs330-lecture-12-notes&quot;&gt;CS330 lecture 1&amp;amp;2 notes&lt;/h1&gt;
&lt;h2 id=&quot;informal-problem-definitions&quot;&gt;Informal Problem Definitions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The multi-task learning problem: Learn all of the tasks more quickly or more proficiently than learning them independently.(更快更专业学习所有任务)&lt;/li&gt;
&lt;li&gt;The meta-learning problem: Given data/experience on previous tasks, learn a new task more quickly and/or more proficiently.（依照先前任务的经验，更快更好的学习新任务）&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
      <category term="meta learning" scheme="https://ereebay.me/tags/meta-learning/"/>
    
  </entry>
  
  <entry>
    <title>A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms 论文笔记【未完】</title>
    <link href="https://ereebay.me/posts/19897/"/>
    <id>https://ereebay.me/posts/19897/</id>
    <published>2020-01-09T08:02:59.000Z</published>
    <updated>2020-03-03T14:08:40.940Z</updated>
    
    <content type="html"><![CDATA[<h1 id="a-meta-transfer-objective-for-learning-to-disentangle-causal-mechanisms">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</h1><h2 id="abstract">Abstract</h2><p>本文提出利用元学习目标，最大限度地提高改变分布的迁移速度，以学习如何模块化获取知识。特别是，我们关注如何在与因果关系一致的情况下将联合分布纳入适当的条件。如果这可行，假设分布的变化是局部化的（distributions are localized）(例如由于对其中一个变量的干预而导致其中一个边缘marginal)。我们证明，在这种假定的因果机制的局部变化的情况下，正确的因果图将趋向于仅有几个具有非零梯度的参数，即需要调整的参数(修改变量的参数)。实验观察到这会导致自适应更快，并利用这一性质来定义一个元学习替代评分，它除了连续的图参数化外，还将有利于正确的因果图。最后，我们考虑到AI智能体方面（例如，机器人自主发现其环境），我们考虑了相同的目标如何能够发现因果变量本身，因为观察到的低水平变量没有因果意义。双变量实例中的实验验证了所提出的思想和理论结果。</p><a id="more"></a><h2 id="introduction">Introduction</h2><p>假设数据是独立同分布的（IID）。同样，模型的性能通常使用来自同一个分布的测试样本进行评估，假设他们代表了所学习系统的使用情况。虽然，从统计的角度对这些假设进行了良好的分析，但是在许多实际情况下难以应用。例如：根据一家医院的历史数据进行训练的医疗诊断系统对来自另一家医院的病人可能表现效果不好，原因是分布情况发生了变化。理论情况下，我们希望我们的模型能够很好地泛化，并且能够<strong>迅速适应</strong>分布外的数据。</p><p>然而，为了能够成功转移到新的分布上，人们就需要更多的数据。在本文中，我们不关注数据分布的假设，而关注数据分布如何变化（例如：从训练分布到转移分布时，可能导致某些agents的action）。我们关注的假设是，当知识用适当的模块化的方式来表示的时候，只有一个或者几个模块发生改变，这些变化是稀疏的。当分布变化是由于一个或多个agnet的action所导致时，这一点尤为重要，因为agent在特定的地点和时间进行干预，这体现在因果关系文献中的讨论的干预措施的形式上，即其中一个因果变量被限制在一个特定值活一个随机变量上。一般来说，agent很难一次影响多个潜在的因果变量，虽然本文不是关于agent 学习本身，但这是我们探索的一个性质，以帮助我们发现这些变量以及他们之间的因果关系。在这个时候，因果图就是一个强大的工具，因为他可以告诉我们干预变量分布中的扰动将如何传播到所有其他变量并影响他们的分布。</p><p>通常情况，因果关系的结构不会提前知道。因果发现的问题通常需要获得因果图，然而这通常只有在强有力的假设下才能实现。一种假设是，已经学会捕捉真正的基础数据生成过程的正确结构的learner应该能够泛化到某种特定方式对结构进行扰动的情况。这可以通过考虑温度和海拔高度的例子来说明：简单来说就是，一个learner通过学习来自于瑞士的数据，对于来自于像荷兰这种山地较少国家的分布数据进行测试的时候，结果仍然有效。因此，建议使用预测模型的分布外的鲁棒性来指导对实际因果结构的推断。</p><p>那么如何利用局部变化的假设呢？正如我们在理论上进行解释并在此处进行实验验证时，如果我们拥有正确的知识的表示，那么从在训练分布上预训练过的模型开始，应该能够很快地适应迁移的数据分布。之所以出现这种情况，是因为我们假设置信数据生成过程是作为独立机制的一部分而获得的，并且从训练分布转到迁移分布时，几乎不需要改变置信机制和参数。因此，获取对应的知识分解的模型仅需要进行一些更新和示例，就可以适应迁移分布。如下所示，在不变的参数上的预期梯度将接近0（如果模型已经在训练分布上很好的训练了），因此在适应迁移分布的过程中，有效搜索空间将大大减少，这可以加快适应的过程，正如实验所体现的那样。<strong>因此，基于正确的知识表示空间的微小变化的假设，我们可以定义一个衡量适应（adaptation）速度的元学习目标，即一种表示后悔（regret）的形式，用于优化知识的表示，分解和结构化的方式</strong></p><p>回到前面温度和海拔高度的例子：如果收到了来自于荷兰的分布外的数据，由于收集了少量来自于荷兰的迁移样本，因此我们期望该模型能够更快的适应。类似于鲁棒性，可以使用自适应速度来知道对于当前问题的真正的因果结构的推断，并可能与因果结构有关的其他信号源一起推断。</p><p><strong>主要的贡献</strong>：我们首先在合成数据上验证，当在真实的二变量因果图（learner不知道）上执行某些干预后，当提供样本数据时，能够正确捕捉到潜在因果结构的模型的适应速度更快。这表明了适应速度可以作为分数充分地评估learner对于基础因果图的拟合程度。然后，我们使用因果图的平滑参数化来以端到端的方式直接优化此分数。最后，我们表明，在未知混合变量的情况下，可以利用分数来区分正确的因果变量。</p><h2 id="which-is-cause-and-which-is-effect">Which is Cause and Which is Effect?</h2><p>To be continued</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;a-meta-transfer-objective-for-learning-to-disentangle-causal-mechanisms&quot;&gt;A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms&lt;/h1&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;本文提出利用元学习目标，最大限度地提高改变分布的迁移速度，以学习如何模块化获取知识。特别是，我们关注如何在与因果关系一致的情况下将联合分布纳入适当的条件。如果这可行，假设分布的变化是局部化的（distributions are localized）(例如由于对其中一个变量的干预而导致其中一个边缘marginal)。我们证明，在这种假定的因果机制的局部变化的情况下，正确的因果图将趋向于仅有几个具有非零梯度的参数，即需要调整的参数(修改变量的参数)。实验观察到这会导致自适应更快，并利用这一性质来定义一个元学习替代评分，它除了连续的图参数化外，还将有利于正确的因果图。最后，我们考虑到AI智能体方面（例如，机器人自主发现其环境），我们考虑了相同的目标如何能够发现因果变量本身，因为观察到的低水平变量没有因果意义。双变量实例中的实验验证了所提出的思想和理论结果。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Improving Federated Learning Personalization via MAML 论文笔记 【未完】</title>
    <link href="https://ereebay.me/posts/6350/"/>
    <id>https://ereebay.me/posts/6350/</id>
    <published>2019-12-02T07:07:30.000Z</published>
    <updated>2020-03-03T14:09:39.706Z</updated>
    
    <content type="html"><![CDATA[<h1 id="improving-federated-learning-personalization-via-maml">Improving Federated Learning Personalization via MAML</h1><h2 id="abstract">Abstract</h2><ol type="1"><li>FL算法与MAML具有很多相似性，可以用元学习算法来对其进行解释</li><li>微调可以使得gloabl 模型具有更强的准确率，同时更容易做定制化处理</li><li>通过标准的中心化数据库训练出来的模型相比Fedavg训练的更难进行定制化处理</li></ol><a id="more"></a><h2 id="introduction">Introduction</h2><ol type="1"><li>指出了FL与MAML算法的联系，并用MAML算法对FL算法进行解释</li><li>对FedAvg进行改进，采用两阶段的训练和fine-tune进行优化</li><li>发现FedAvg其实本质是一种metalearning算法，用于优化个性化定制的效果，而不是全局模型的优化。</li></ol><h2 id="interpreting-fedavg-as-a-meta-learning-algorithm">Interpreting FedAvg as a Meta Learning Algorithm</h2><p>下图展现了在FL中应用MAML算法（左侧），Reptile算法（中间）和FL的训练算法FedAvg（右侧）。设L为损失函数，在每一轮的迭代中，MAML会通过随机采样一个batch的任务T来进行训练。对于每个任务T，会有一个内循环，然后在外循环中聚集每个任务所获得的的梯度更新。对于FL算法会随机采样数个client T。对于每个T和其权重，会在local数据上进行数轮的迭代优化，然后将更新的梯度聚集形成一个新的global model。如果我们简化设置，并认为所有的client拥有相同的数据，那么所有的权重就会一样，这个时候reptile和fedavg其实就是同一种算法。</p><p><img src="http://cdn.ereebay.me/blog/fl-maml/fl-maml-1.png"></p><p>假设在FedAvg中的权重相同为wi。考虑有T个clients，并设置每个相关模型参数为<span class="math inline">\(\theta\)</span>。对于每个cilent i，其损失函数为<span class="math inline">\(L_{i}(\theta)\)</span>,记<span class="math inline">\(g_{j}^{i}\)</span>为第<span class="math inline">\(j^{t h}\)</span>local训练过程所计算得到的梯度。</p><p>FedSGD的梯度更新函数为：</p><p><span class="math display">\[g_{F e d S G D}=\frac{-\beta}{T} \sum_{i=1}^{T} \frac{\partial L_{i}(\theta)}{\partial \theta}=\frac{1}{T} \sum_{i=1}^{T} g_{1}^{i}\]</span></p><p>将设我们将FOMAML用相同的术语来表示。假设Client 学习率为<span class="math inline">\(\beta\)</span>, 每个client的个性化模型经过K步后所获得的梯度更新为<span class="math inline">\(\theta_{K}^{i}=U_{K}^{i}(\theta)=\theta-\beta \sum_{j=1}^{K} g_{j}^{i}=\theta-\beta \sum_{j=1}^{K} \frac{\partial L_{i}\left(\theta_{j}\right)}{\partial \theta}\)</span></p><p>求微分可得到：</p><p><span class="math display">\[\frac{\partial U_{K}^{i}(\theta)}{\partial \theta}=I-\beta \frac{\partial \sum_{j=1}^{K} g_{j}^{i}}{\partial \theta}=I-\beta \sum_{j=1}^{K} \frac{\partial^{2} L_{i}\left(\theta_{j}\right)}{\partial \theta^{2}}\]</span></p><p>在进行K次梯度更新后，对整个模型进行更新：</p><p><span class="math display">\[g_{M A M L}=\frac{\partial L_{M A M L}}{\partial \theta}=\frac{1}{T} \sum_{i=1}^{T} \frac{\partial L_{i}\left(U_{K}^{i}(\theta)\right)}{\partial \theta}=\frac{1}{T} \sum_{i=1}^{T} L_{i}^{\prime}\left(U_{K}^{i}(\theta)\right)\left(I-\beta \sum_{j=1}^{K} \frac{\partial^{2} L_{i}\left(\theta_{j}\right)}{\partial \theta^{2}}\right)\]</span></p><p>为了避免二次求导带来的计算量问题，FOMAML应运而生，通过K次的梯度更新后，直接采用第K+1次的梯度更新作为local update。</p><p><span class="math display">\[g_{F O M A M L}(K)=\frac{1}{T} \sum_{i=1}^{T} L_{i}^{\prime}\left(U_{K}^{i}(\theta)\right) I=\frac{1}{T} \sum_{i=1}^{T} L_{i}^{\prime}\left(\theta_{K}^{i}\right)=\frac{1}{T} \sum_{i=1}^{T} g_{K+1}^{i}\]</span></p><p>通过上面的公式，不难看出，其实FedAvg的更新，所有client的更新的平均，其实就是以上两种idea的线性组合。</p><p><span class="math display">\[g_{F e d A v g}=\frac{1}{T} \sum_{i=1}^{T} \sum_{j=1}^{K} g_{j}^{i}=\frac{1}{T} \sum_{i=1}^{T} g_{1}^{i}+\sum_{j=1}^{K-1} \frac{1}{T} \sum_{i=1}^{T} g_{j+1}^{i}=g_{F e d S G D}+\sum_{j=1}^{K-1} g_{F O M A M L}(j)\]</span></p><h2 id="personalized-fedavg">Personalized FedAvg</h2><p><img src="http://cdn.ereebay.me/blog/fl-maml/fl-maml-2.png"></p><p>如上图所示，采用算法1中的FedAvg E训练E个local epoch，根据local数据量来对梯度更新进行权衡。然后在FL的环境下采用Retile（K）训练K个local steps，不考虑本地的数据量。</p><p>一般来说，就通信轮次的数量而言，FedAvg训练数个local epochs后，可以在数轮通信内就能快速收敛。由于生产环境的复杂性，这种测量方式被用于衡量FL算法的收敛速度。本文发现，采用momentum SGD的方法作为server优化器已经对于personalized model进行了优化，然而initial model相对不稳定。以前的方法是减少本地的训练轮次或者学习率。</p><p>本文提出采用Retile（K）的方法进行fintune，然后用Adam作为server优化器，来提升initialmodel的效果。同时可以稳定personalized model。</p><p>To be continued</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;improving-federated-learning-personalization-via-maml&quot;&gt;Improving Federated Learning Personalization via MAML&lt;/h1&gt;
&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;FL算法与MAML具有很多相似性，可以用元学习算法来对其进行解释&lt;/li&gt;
&lt;li&gt;微调可以使得gloabl 模型具有更强的准确率，同时更容易做定制化处理&lt;/li&gt;
&lt;li&gt;通过标准的中心化数据库训练出来的模型相比Fedavg训练的更难进行定制化处理&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>Federated Adversarial Domain Adaptation 论文笔记【未完】</title>
    <link href="https://ereebay.me/posts/62459/"/>
    <id>https://ereebay.me/posts/62459/</id>
    <published>2019-11-24T06:04:02.000Z</published>
    <updated>2020-03-03T14:09:27.587Z</updated>
    
    <content type="html"><![CDATA[<h1 id="federated-adversarial-domain-adaptation">Federated Adversarial Domain Adaptation</h1><p>目前联邦学习存在知识域迁移的问题，而导致无法具有较好的泛化能力。当源节点的标记数据和目标节点的未标记数据不同时候，就会出现域迁移的情况。</p><p>这paper主要提出了一种方式来解决联邦学习中知识域使用的方法，为了能够将不同节点所学习的知识能够和目标节点的数据分布所对齐。</p><p>该论文的方法主要将对抗适应技术应用到了联邦学习中。此外，设计了一种动态注意力机制，并且利用特征分解来增强知识迁移。</p><a id="more"></a><h2 id="introduction">Introduction</h2><p>传统的联邦学习存在一个问题：由于每个节点上的数据是通过nonidd的行为所收集而来的，因此会产生一个domain shift 的问题。例如：一个设备拍到的图片很多都是室内场景，一个拍到的很多是室外场景。本文主要提出的一个方法是，<strong>在不需要额外用户的监督情况下，通过对去中心化的节点中的数据进行知识迁移到一个新的不同的数据域的节点。</strong> 该方法也被称为，Unsupervised Federated Domain Adaptation.</p><p>目前有许多非监督域适应的方法，但是因为联邦学习的背景导致了一下的问题：</p><ol type="1"><li>数据的存放是locally，而且无法分享。</li><li>模型的参数是在不同节点上独立训练，并且以不同的速度进行收敛，对于global模型的贡献取决于这两者域的相关性。</li><li>由于通过源节点学习出来的知识是高度集成的，难以解构，可能会引起negative transfer。</li></ol><p>本文主要提出的方法名称为Federated Adversarial Domain Adaptation(FADA)，利用对抗技术在联邦学习系统中解决域迁移的问题。</p><p>方法的主要步骤分为：</p><ol type="1"><li>首先从理论角度对联邦域适应问题进行分析，并且提供一个泛化的，通用的界限。</li><li>收到理论效果的启发，提出一种有效的自适应算法，该算法基于对抗性适应和应用于联邦学习环境的表征解构技术。</li><li>设计一个动态注意力模型来应对联邦学习系统中不断变化的收敛速率。</li></ol><figure><img src="http://cdn.ereebay.me/FADAfada-1.png" alt="png1"><figcaption>png1</figcaption></figure><h2 id="realated-work">Realated Work</h2><p><strong>Unsupervised Domain Adaptation</strong>: UDA的目的是将知识从已经标记的数据域迁移到一个未标记的数据域。</p><p><strong>Federated Learning</strong>:不多介绍了</p><p><strong>Feature Disentanglement</strong>:特征解构，神经网络通过复杂的隐藏层提取出来的特征，通常是高度集成的。因此通过学习解构的特征可以帮助一出一些不相关，或者是特定域的知识，然后对需要的域知识进行建模。</p><h2 id="generalization-bound-for-fda">Generalization Bound for FDA</h2><ol type="1"><li>首先回顾一下以单数据源为背景的自适应问题的理论误差范围的定义</li><li>然后描述在无监督的联邦域自适应情况下的误差范围的定义</li></ol><h2 id="federated-adversarial-domain-adaptation-1">Federated Adversarial Domain Adaptation</h2><p><img src="http://cdn.ereebay.me/flpaper-2.png"></p><p>根据上一章节的理论不难看出权重<span class="math inline">\(\alpha\)</span>和距离的重要性。本文的方法是通过提出一个动态的注意力机制进行权重的学习，通过联邦对抗对齐机制来缩小源域和目标域的距离。此外，还采用了特征解构的方法提取和域无关的特征来加强知识的迁移。</p><p><strong>动态注意力机制</strong>：</p><p>该机制的原理主要是去提高那些贡献度高的节点权重，降低贡献度低的节点权重，那么如何判断节点的贡献程度呢？本文采用了gap statistics方法评估目标特征能够多好的被clustered。</p><p><span class="math display">\[I=\sum_{r=1}^{k} \frac{1}{2 n_{r}} \sum_{i, j \in C_{r}}\left\|f_{i}^{t}-f_{j}^{t}\right\|_{2}\]</span></p><p>假设有<span class="math inline">\(C_{1}, C_{2}, \ldots, C_{k}\)</span>这么多聚集，<span class="math inline">\(C_r\)</span>表示第r个聚集中，对象的索引，而且<span class="math inline">\(n_{r}=\left|C_{r}\right|\)</span>。</p><p>直觉上来说，一个更小的gap statistics值说明了特征分布拥有更小的类内方差。通过计算两次迭代的gap statistics的差值来衡量每个源节点贡献的程度。</p><p><span class="math display">\[I_{i}^{g a i n}=I_{i}^{p-1}-I_{i}^{p}\]</span></p><p>它表示的是在目标域采用源域的梯度更新后，clusters可以提升多少。对于梯度的更新采用，来决定每个梯度贡献多少</p><p><span class="math display">\[\text { Softmax }\left(I_{1}^{\text {gain }}, I_{2}^{\text {gain }}, \ldots, I_{N}^{\text {gain}}\right)\]</span></p><p><strong>联邦对抗对齐</strong>：</p><p>机器学习的模型会因为域距离，使得模型效果大大减弱。为解决这个问题，本文方法在联邦学习的环境中，改善了传统的对抗训练方法。在联邦学习的背景中，本文将对抗对齐优化过程分为两个步骤，1. 对于每个域，训练一个local的特征提取器，<span class="math inline">\(G_i\)</span>和<span class="math inline">\(G_t\)</span>分别对应<span class="math inline">\(D_i\)</span>和<span class="math inline">\(D_t\)</span>。2.对于每个（<span class="math inline">\(D_i, D_t\)</span>）源-目标域对，训练一个对抗域鉴别器DI，来采用对抗学习的方式将两个分布进行对齐。首先训练DI识别特征来自于哪个特征域，然后训练生成器也就是特征提取器（<span class="math inline">\(G_i, G_t\)</span>）来迷惑DI。注意：D只能获得<span class="math inline">\(G_i\)</span>和<span class="math inline">\(G_t\)</span>的输出。</p><p>在给定第i个源数据域<span class="math inline">\(\mathbf{X}^{S_{i}}\)</span>，目标域<span class="math inline">\(\mathbf{X}^T\)</span>，<span class="math inline">\(D I_{i}\)</span>的优化目标为：</p><p><span class="math display">\[\underset{\Theta^{D I_{i}}{L}}{L_{a d v_{D I}}\left(\mathbf{X}^{S_{i}}, \mathbf{X}^{T}, G_{i}, G_{t}\right)=-\mathbb{E}_{\mathbf{x}^{s_{i} \sim \mathbf{X}^{s_{i}}}}\left[\log D I_{i}\left(G_{i}\left(\mathbf{x}^{s_{i}}\right)\right)\right]-\mathbb{E}_{\mathbf{x}^{t} \sim \mathbf{x}^{t}}\left[\log \left(1-D I_{i}\left(G_{t}\left(\mathbf{x}^{t}\right)\right)\right)\right]}\]</span></p><p>然后维持D不变，更新G的目标函数：</p><p><span class="math display">\[\underset{\Theta^{G}{G}_{i}, \Theta^{G_{t}}}{L}\left(\mathbf{X}^{S_{i}}, \mathbf{X}^{T}, D I_{i}\right)=-\mathbb{E}_{\mathbf{x}^{s_{i} \sim \mathbf{X}^{s_{i}}}\left[\log D I_{i}\left(G_{i}\left(\mathbf{x}^{s_{i}}\right)\right)\right]-\mathbb{E}_{\mathbf{x}^{t} \sim \mathbf{X}^{t}}\left[\log D I_{i}\left(G_{t}\left(\mathbf{x}^{t}\right)\right)\right]}\]</span></p><p><strong>特征解构</strong>：</p><p>本文在对抗学习框架之下同时还采用了对抗解构的方式提取和域不变的特征。本文认为可以将提取的特征分为域特定和域不变特征。正如第一张图所示，特征解构器D会将特征分为两种类别，首先训练一个K分类的分类器，和K分类的类别识别器来根据特征预测标签。</p><p><span class="math display">\[\begin{array}{c}{L_{cross entropy}} \\ {\Theta^{G_i}, \Theta^{D_i}, \Theta^{C_i}, \Theta^{CI_i}}\end{array}=-\mathbb{E}_{\left(\mathbf{x}^{s_i}, \mathbf{y}^{s_i}\right) \sim \widehat{\mathcal{D}}_{s_i}} \sum_{k=1}^{K} \mathbb{1}\left[k=\mathbf{y}^{s_{i}}\right] \log \left(C_{i}\left(f_{d i}\right)\right)-\mathbb{E}_{\left(\mathbf{x}^{s} i, \mathbf{y}^{s} i\right) \sim \widehat{\mathcal{D}}_{s_{i}}} \sum_{k=1}^{K} \mathbb{1}\left[k=\mathbf{y}^{s_{i}}\right] \log \left(C I_{i}\left(f_{d s}\right)\right)\]</span></p><p>然后固定类别分类器，通过产生域特定特征来仅训练特征结构器，来迷惑类别分类器。</p><p><span class="math display">\[\underset{\Theta^{D} i, \Theta^{G_{i}}}{L_{e n t}}=-\frac{1}{N_{s_{i}}} \sum_{j=1}^{N_{s_{i}}} \log C I_{i}\left(f_{d s}^{j}\right)=-\frac{1}{N_{s_{i}}} \sum_{j=1}^{N_{s_{i}}} \log C I_{i}\left(D_{i}\left(G_{i}\left(\mathbf{x}^{s_{i}}\right)\right)\right)\]</span></p><p>特征解构方法通过保留类不变特征，和去除类特定特征来进行知识的迁移，为了增强解构的效果，本文通过最小化两者的共同信息。</p><p>共同信息定义为：<span class="math inline">\(I\left(f_{d i} ; f_{d s}\right)=\int_{\mathcal{P} \times \mathcal{Q}} \log \frac{d \mathbb{P}_{\mathcal{P} Q}}{d \mathbb{P}_{\mathcal{P}} \otimes \mathbb{P}_{\mathcal{Q}}} d \mathbb{P}_{\mathcal{P} \mathcal{Q}}\)</span>，其中<span class="math inline">\(\mathbb{P}_{\mathcal{P} \mathcal{Q}}\)</span>是两种特征的联合概率分布，<span class="math inline">\(\mathbb{P}_{\mathcal{P}}=\int_{\mathcal{P}} d \mathbb{P}_{\mathcal{P} \mathcal{Q}}, \mathbb{P}_{\mathcal{Q}}=\int_{\mathcal{Q}} d \mathbb{P}_{\mathcal{P} \mathcal{Q}}\)</span>分别是对应的边缘概率分布。</p><p>采用Mutual Information Neural Estimator（MINE）来评估互信息：</p><p><span class="math display">\[T_{\theta}: \widehat{I(\mathcal{P} ; \mathcal{Q})}_{n}=\sup _{\theta \in \Theta} \mathbb{E}_{\mathbb{P}_{\mathcal{P} Q}^{(n)}}\left[T_{\theta}\right]-\log \left(\mathbb{E}_{\mathbb{P}_{P}^{(n)} \otimes \mathbb{P}_{Q}^{(n)}}\left[e^{T_{\theta}}\right]\right)\]</span></p><p>实际情况下可以通过下式进行计算：</p><p><span class="math display">\[I(\mathcal{P} ; \mathcal{Q})=\iint \mathbb{P}_{\mathcal{P} \mathcal{Q}}^{n}(p, q) T(p, q, \theta)-\log \left(\iint \mathbb{P}_{\mathcal{P}}^{n}(p) \mathbb{P}_{\mathcal{Q}}^{n}(q) e^{T(p, q, \theta)}\right)\]</span></p><p>为了避免计算积分，采用门特卡罗法还避免计算积分：</p><p><span class="math display">\[I(\mathcal{P}, \mathcal{Q})=\frac{1}{n} \sum_{i=1}^{n} T(p, q, \theta)-\log \left(\frac{1}{n} \sum_{i=1}^{n} e^{T\left(p, q^{\prime}, \theta\right)}\right)\]</span></p><p>其中（p，q）采样自于联合分布，<span class="math inline">\(q^{\prime}\)</span>采样与边缘分布，<span class="math inline">\(T(p, q, \theta)\)</span>是由<span class="math inline">\(\theta\)</span>决定的神经网络，来评估P和Q分布的互信息。</p><p>域无关特征和域特定特征会输入到重建器中得到重组的特征，loss函数采用l2损失，来重建原来特征，为了保持表示的可组成型。L2约束和互信息的损失约束可以通过调整超参数来平衡。</p><p>To be continued</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;federated-adversarial-domain-adaptation&quot;&gt;Federated Adversarial Domain Adaptation&lt;/h1&gt;
&lt;p&gt;目前联邦学习存在知识域迁移的问题，而导致无法具有较好的泛化能力。当源节点的标记数据和目标节点的未标记数据不同时候，就会出现域迁移的情况。&lt;/p&gt;
&lt;p&gt;这paper主要提出了一种方式来解决联邦学习中知识域使用的方法，为了能够将不同节点所学习的知识能够和目标节点的数据分布所对齐。&lt;/p&gt;
&lt;p&gt;该论文的方法主要将对抗适应技术应用到了联邦学习中。此外，设计了一种动态注意力机制，并且利用特征分解来增强知识迁移。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>MemoryGAN 论文笔记【未完】</title>
    <link href="https://ereebay.me/posts/64315/"/>
    <id>https://ereebay.me/posts/64315/</id>
    <published>2019-04-22T06:05:02.000Z</published>
    <updated>2020-01-11T13:37:29.025Z</updated>
    
    <content type="html"><![CDATA[<h1 id="memory-gan-阅读笔记">Memory GAN 阅读笔记</h1><h2 id="简介">简介</h2><p>本文主要解决了训练非监督GAN中的两个问题，</p><p>第一，由于生成对抗网络只使用连续潜在分布来表示多个类或者数据簇，因此他们通常没办法正确的处理潜在空间中不同类之间的结构不连续性质。(由于模式崩溃问题导致) 例如 GAN 吧建筑和猫 嵌入在同一个连续潜在分布中 因此GAN可能在两个类别的过渡区域中 生成真实图像。</p><p>其次， 生成对抗网络的鉴别器非常容易遗忘过去生成的样本，在对抗训练过程中会产生不稳定性。</p><p>作者认为这两个主要问题可以通过生成器鉴别器都能访问的可学习的记忆网络得到解决。生成器可以有效学习训练样本的表示，以理解数据的底层聚类分布，从而缓解结构的不连续问题。与此同时，鉴别器可以更好记忆先前生成的样本的集群，这可以减轻遗忘的问题。</p><p>本文提出了一种端到端的生成对抗网络模型 记忆GAN，该模型涉及一种无监督的并且和现有生成对抗网络模型继集成的存储网络。</p><a id="more"></a><p>Von Mises-Fisher (vMF) mixture model. 记忆模块能够有效地缓解不稳定的问题。首先，为缓解结构不连续性问题，内存可以学习训练样本的表示，帮助生成器更好理解类和集簇分布。因此，我们可以将离散簇的建模与连续潜在空间上的数据属性的嵌入分开，这可以减轻不连续性问题。</p><p>其次 记忆网络能够通过学习记忆先前生成的样本数据簇来缓解遗忘问题，包括那种很稀有的样本。</p><h2 id="结构">结构</h2><p>总体结构分为 记忆鉴别网络DMN和记忆条件生成网络（MCGN）</p><h3 id="记忆鉴别网络">记忆鉴别网络</h3><p>记忆鉴别网络分别由一个前馈网络<span class="math inline">\(\mu\)</span> 和一个记忆网络组成。</p><p>其中前馈网络 <span class="math inline">\(\mu\)</span> 是卷积神经网络，输入为<span class="math inline">\(x \in \mathbb{R}^{D}\)</span>，输出是一个标准化后的向量 <span class="math inline">\(q=\mu(x) \in \mathbb{R}^{M}\)</span>，其中 <span class="math inline">\(\|q\|=1\)</span>。然后作为记忆模块的输入并输出判断结果。</p><p>记忆网络的公式为：<span class="math inline">\(\mathcal{M}=(K, v, a, h)\)</span>，其中<span class="math inline">\(K \in \mathbb{R}^{N \times M}\)</span> 是内存key矩阵，<span class="math inline">\(N\)</span>是内存尺寸而<span class="math inline">\(M\)</span>是维度。<span class="math inline">\(v \in\{0,1\}^{N}\)</span> 是内存矩阵。从概念上将，每个关键向量存储通过vMF混合模型学习的聚类中心表示，并且其对应的关键值是真假的判断结果。<span class="math inline">\(a \in \mathbb{R}^{N}\)</span> 是表示存在每个内存块中的物体的时长。而<span class="math inline">\(h \in \mathbb{R}^{N}\)</span>表示的是slot 直方图，其中每个<span class="math inline">\(h_{i}\)</span>表示属于第i个内存槽中的有效数据量。</p><h3 id="相关机制">相关机制：</h3><p>life-long memory network：可以自由增长内存空间，训练过程中无需重置。</p><p>k-nearest neighbor indexing for efficient memory lookup: k近邻索引用于查询</p><p>least recently used (LRU) scheme for memory update. LRU机制用于内存更新</p><h3 id="创新">创新：</h3><ul><li>概率推倒：可以计算数据极大似然，内存索引的范畴分布的先验分布后验分布。</li><li>memory通过使用增量EM算法最大化似然来学习query的近似分布。</li><li>通过GANloss优化memory而不是memoryloss优化</li><li>通过跟踪slot histogram 来决定每个样本的贡献程度</li></ul><h3 id="鉴别器输出">鉴别器输出</h3><p>对于每个输入<span class="math inline">\(x\)</span>， 需要先决定哪个memory slot 来计算discriminative probability</p><p><span class="math inline">\(c \in\{1,2, \ldots, N\}\)</span> 表示 memory slot index。</p><p>采用Von Mises-Fisher(vMF)混合模型表示memory index 的后验分布。</p><p><span class="math display">\[p(c=i | x)=\frac{p(x | c=i) p(c=i)}{\sum_{j=1}^{N} p(x | c=j) p(c=j)}=\frac{\exp \left(\kappa K_{i}^{T} \mu(x)\right) p(c=i)}{\sum_{j=1}^{N} \exp \left(\kappa K_{j}^{T} \mu(x)\right) p(c=j)}\]</span></p><p><span class="math inline">\(p(x | c=i)=C(\kappa) \exp \left(\kappa K_{i}^{T} \mu(x)\right)\)</span>中 <span class="math inline">\(\kappa=1\)</span> 为常量注意力参数。</p><p>vMF实际上等效于在单位球面上定义的正确归一化的高斯分布</p><p>关于memory index的范畴分布 <span class="math inline">\(p(c)\)</span> 通过归一化 slot histogram <span class="math inline">\(p(c=i)=\frac{h_{i}+\beta}{\sum_{j=1}^{N}\left(h_{j}+\beta\right)}\)</span>，其中<span class="math inline">\(\beta\left(=10^{-8}\right)\)</span>是一个平滑常量用于数值稳定性。通过使用<span class="math inline">\(p(y=1 | c=i, x)=v_{i}\)</span>，我们在c上求边缘化联合概率<span class="math inline">\(p(y=1, c | x)\)</span> 得到<span class="math inline">\(p(y=1 | x)\)</span>：</p><p><span class="math display">\[p(y=1 | x)=\sum_{i=1}^{N} p(y=1 | c=i, x) p(c=i | x)=\sum_{i=1}^{N} v_{i} p(c=i | x)=\mathbb{E}_{i \sim p(c | x)}\left[v_{i}\right]\]</span></p><p>然而对于每个样本x，对于整个大小为N的memory是不可扩展的。</p><p>采用最大后验概率来考虑top-k slots <span class="math inline">\(S=\left\{s_{1}, \dots, s_{k}\right\}\)</span></p><p><span class="math display">\[S=\underset{c_{1}, \ldots, c_{k}}{\operatorname{argmax}} p(c | x)=\underset{c_{1}, \ldots, c_{k}}{\operatorname{argmax}} p(x | c) p(c)=\underset{c_{1}, \ldots, c_{k}}{\operatorname{argmax}} \exp \left(\kappa K_{c}^{T} \mu(x)\right)\left(h_{c}+\beta\right)\]</span></p><p>其中<span class="math inline">\(p(x | c)\)</span>是vMF似然，<span class="math inline">\(p(c)\)</span>是memory index的先验分布。</p><p>在这我们省略了vMF似然和先验坟墓的归一化，因为他在都是常数。一旦我们获得了S，就可以求得</p><p><span class="math display">\[p(y | x) \approx \frac{\sum_{i \in S} v_{i} p(x | c=i) p(c=i)}{\sum_{j \in S} p(x | c=j) p(c=j)}\]</span></p><h3 id="内存更新机制">内存更新机制</h3><p>memory keys and values 在训练过程中会进行更新。更新机制包括传统的内存更新机制和增量EM算法。</p><p>设样本为<span class="math inline">\(x\)</span>标签<span class="math inline">\(y\)</span>真为1假为0。对于每一个x，首先寻找k-nearest slots <span class="math inline">\(S_{y}\)</span>，但是采用条件后验<span class="math inline">\(p\left(c | x, v_{c}=y\right)\)</span>。 这是为了能够在接下去的EM算法过程中，只考虑与y属于同一个类别的slots。</p><p>后面我们就根据<span class="math inline">\(S_y\)</span>中是否包含正确的标签来根据不同方式更新memory。</p><h4 id="没有正确label">没有正确label：</h4><p>通过<span class="math inline">\(n_{a}=\operatorname{argmax}_{i \in\{1, \ldots, N\}} a_{i}\)</span>寻找最老的memory slot，并将对x的信息复制到对应位置。<span class="math inline">\(K_{n_{a}} \leftarrow q=\mu(x)\)</span>，<span class="math inline">\(v_{n_{a}} \leftarrow y\)</span>，<span class="math inline">\(a_{n_{a}} \leftarrow 0\)</span>，<span class="math inline">\(h_{n_{a}} \leftarrow \frac{1}{N} \sum_{i=1}^{N} h_{i}\)</span>。</p><h4 id="有正确label">有正确label：</h4><p>通过以下用于T次迭代的自定义的增量EM算法，部分包括新样本的信息来更新memory key。</p><p>在推断过程中，通过将前一时刻的keys<span class="math inline">\(\hat{K}_{i}^{t-1}\)</span>和<span class="math inline">\(\hat{h}_{i}^{t-1}\)</span>应用到公式1，计算后验分布<span class="math inline">\(\gamma_{i}^{t}=p\left(c_{i} | x\right)\)</span>，其中<span class="math inline">\(i \in S_{y}\)</span>。在最大化过程中，将进行如下更新：</p><p><span class="math display">\[\hat{h}_{i}^{t} \leftarrow \hat{h}_{i}^{t-1}+\gamma^{t}-\gamma^{t-1}, \quad \hat{K}_{i}^{t} \leftarrow \hat{K}_{i}^{t-1}+\frac{\gamma^{t}-\gamma^{t-1}}{\hat{h}_{i}^{t}}\left(q_{i}-\hat{K}_{i}^{t}\right)\]</span></p><p>其中，<span class="math inline">\(t \in 1, \ldots, T, \gamma^{0}=0, \hat{K}_{i}^{1}=K_{i}, \hat{h}_{i}^{1}=\alpha h_{i}\)</span>，<span class="math inline">\(\alpha=0.5\)</span></p><p>经过T次迭代，通过<span class="math inline">\(K_{i} \leftarrow \hat{K}_{i}^{t}\)</span>和<span class="math inline">\(h_{i} \leftarrow \hat{h}_{i}^{t}\)</span>更新<span class="math inline">\(S_y\)</span>的slots。</p><p>衰减率<span class="math inline">\(\alpha\)</span>控制它以指数方式减少就查询对混合分量的平均方向的slot位置的贡献的程度。</p><p><span class="math inline">\(\alpha\)</span>对于性能至关重要，因为用于更新keys的旧序列不适合当前的混合粉不，因为前馈网络本身也在进行更新。</p><p>最终，值得注意的是，这种内存更新机制和对抗训练算法是正交的，因为当鉴别器更新的时候，是内存的更新是独立执行的。此外，添加内存模块不会影响模型在测试的速度，因为memory只在训练的时候更新</p><h2 id="记忆条件生成网络">记忆条件生成网络</h2><p>记忆条件生成网络基于InfoGAN的生成器。但是区别在于他不仅以随机噪声为条件，同样对memory信息为条件。</p><p>换言之，生成器不仅从噪声分布中随机采样枣红色呢过，同时还会从<span class="math inline">\(P\left(c=i | v_{c}=1\right)=\frac{h_{i} v_{i}}{\sum_{j}^{N} h_{j} v_{j}}\)</span>采样memory index <span class="math inline">\(i\)</span>。上面的公式表示的是存储真实数据的单元i出现的频率。最终的输入是<span class="math inline">\([K_i, z]\)</span>，<span class="math inline">\(K_i\)</span>是memory index i的key vector</p><p>与其他CGAN的区别是，MCGN不需要额外的标注或者是额外的encoder。相反，MCGN可以充分利用DMN通过非监督形式学习到的memory信息。DMN仅采用序列中每个样本和其标签来学习vMF混合memory</p><p>整个memorygan的训练过程是</p><p>for 训练迭代次数 do 从训练样本中采样一个minibatch的样本 从噪声分布和memory index 随机采样一个minibatch 更新鉴别器的loss 寻找minibatch中每个数据的Sy 对Sy中的每个key h gama 进行初始化 for EM迭代次数 do 估计每个s的<span class="math inline">\(\gamma_s\)</span> 更新<span class="math inline">\(h_s\)</span> 更新<span class="math inline">\(K_s\)</span> 更新 vMF混合模型， <span class="math inline">\(h_{s_{y}} \leftarrow \hat{h}_{s_{y}}^{T}, K_{s_{y}} \leftarrow \hat{K}_{s_{y}}^{T}\)</span> for <span class="math inline">\(s_{y} \in S_{y}\)</span> 从噪声分布和memory index 随机采样一个minibatch 更新生成器loss</p><h2 id="objective-function">objective function</h2><p>memoryGAN的目的是基于InfoGAN的目的，是为了最大化潜在变量和观察内容之间的共同信息。（这个可以查阅infoGAN）</p><p>在<span class="math inline">\(K_i\)</span>和<span class="math inline">\(G(z, K_i)\)</span>之间增加一个共同信息loss，来保证采样的memory 信息和生成样本的结构化信息的连续性：</p><p><span class="math display">\[I\left(K_{i} ; G\left(z, K_{i}\right)\right) \geq H\left(K_{i}\right)-\hat{I}-\log C(\kappa)\]</span></p><p>其中，<span class="math inline">\(\hat{I}\)</span> 表示负的余弦相似度的期望 <span class="math inline">\(\hat{I}=-E_{x \sim G\left(z, K_{i}\right)}\left[\kappa K_{i}^{T} \mu(x)\right]\)</span></p><p>To be continued</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;memory-gan-阅读笔记&quot;&gt;Memory GAN 阅读笔记&lt;/h1&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;本文主要解决了训练非监督GAN中的两个问题，&lt;/p&gt;
&lt;p&gt;第一，由于生成对抗网络只使用连续潜在分布来表示多个类或者数据簇，因此他们通常没办法正确的处理潜在空间中不同类之间的结构不连续性质。(由于模式崩溃问题导致) 例如 GAN 吧建筑和猫 嵌入在同一个连续潜在分布中 因此GAN可能在两个类别的过渡区域中 生成真实图像。&lt;/p&gt;
&lt;p&gt;其次， 生成对抗网络的鉴别器非常容易遗忘过去生成的样本，在对抗训练过程中会产生不稳定性。&lt;/p&gt;
&lt;p&gt;作者认为这两个主要问题可以通过生成器鉴别器都能访问的可学习的记忆网络得到解决。生成器可以有效学习训练样本的表示，以理解数据的底层聚类分布，从而缓解结构的不连续问题。与此同时，鉴别器可以更好记忆先前生成的样本的集群，这可以减轻遗忘的问题。&lt;/p&gt;
&lt;p&gt;本文提出了一种端到端的生成对抗网络模型 记忆GAN，该模型涉及一种无监督的并且和现有生成对抗网络模型继集成的存储网络。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>kettle入门教程</title>
    <link href="https://ereebay.me/posts/5052/"/>
    <id>https://ereebay.me/posts/5052/</id>
    <published>2019-03-23T04:44:49.000Z</published>
    <updated>2019-03-23T04:45:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kettle-入门教程">kettle 入门教程</h1><p>由于最后一个学期还有一个学分不够，就得再休一门课，看了看课表也就只能挑个OLAP了，又是一门完全陌生的课，然后网上关于kettle的教程又非常少，自己就搜罗了一些资料记录一下该课需要用的工具的笔记吧</p><a id="more"></a><h2 id="kettle-介绍">kettle 介绍</h2><p>kettle是一个ETL（Extract, Transform and Load）数据抽取、转换、载入工具，ETL工具在数据仓库项目使用非常频繁，kettle也可以应用在以下一些场景：</p><ul><li><p>在不同应用或数据库之间整合数据</p></li><li><p>把数据库中的数据导出到文本文件</p></li><li><p>大批量数据装载入数据库</p></li><li><p>数据清洗</p></li></ul><p>集成应用相关项目是个使用</p><p>kettle使用非常简单，通过图形界面设计实现做什么业务，无需写代码去实现，因此,kettle是以面向元数据来设计</p><p>kettle支持很多种输入和输出格式，包括文本文件，数据表，以及商业和免费的数据库引擎。另外，kettle强大的转换功能让您非常方便操纵数据。</p><h2 id="kettle-安装">kettle 安装</h2><p><span class="exturl" data-url="aHR0cHM6Ly9jb21tdW5pdHkuaGl0YWNoaXZhbnRhcmEuY29tL2RvY3MvRE9DLTEwMDk4NTU=" title="https://community.hitachivantara.com/docs/DOC-1009855">下载地址<i class="fa fa-external-link"></i></span></p><p>由于我的平台是mac，有bug，双击不能运行，只能解压后 在终端输入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh spoon.sh</span><br></pre></td></tr></table></figure><p>运行spoon</p><h2 id="数据转换-导出为excel">数据转换-导出为Excel</h2><ol type="1"><li>连接数据库</li><li>导出为Excel</li></ol><p>首先下图所示新建一个转换，并设置数据库的连接。</p><p><img src="/images/kettle/kettle-1.png"></p><p><img src="/images/kettle/kettle-2.png"></p><p>设置完数据库连接之后，就可以设置输入源，这里我们采用数据库中的table作为输入源</p><p><img src="/images/kettle/kettle-3.png"></p><p>设置好输入源可以点击preview先预览数据</p><p><img src="/images/kettle/kettle-4.png"></p><p>设置完输入源 同理将输出源选择excel拖到工作界面，但是先别急着配置输出源，在view栏，添加一个hop，用来连接输入和输出源</p><p><img src="/images/kettle/kettle-5.png"></p><p>这样在配置输出源的时候，在字段的tab就可以直接获取选择要导出的字段</p><p><img src="/images/kettle/kettle-6.png"></p><p>最后点击开始就进行了转换</p><p><img src="/images/kettle/kettle-7.png"></p><p>最后来看下导出的文件</p><p><img src="/images/kettle/kettle-8.png"></p><h2 id="hello-world">Hello World</h2><p>好，现在你已经学会了转换，下面我们就来个Helloword实例，这个教程是我在网上看到的，觉得还不错，可以多接触kettle的一些功能，而不像我上面的非常基础。</p><p>那么这个helloworld就是针对数据库中的每个人名输出对应的gretting并导出为文件。</p><p>首先同理设置输入源，但SQL语句需要修改下，我这里是只选出了他们的名字，按id排列，取前十个。</p><p>然后就是写个js脚本自动添加个hello</p><p><img src="/images/kettle/kettle-9.png"></p><p>最后用hop将三个step链接，可以先执行preview看下结果</p><p><img src="/images/kettle/kettle-10.png"></p><p><img src="/images/kettle/kettle-11.png"></p><p>最后可以看下结果</p><p><img src="/images/kettle/kettle-12.png"></p><p>以上只是演示了数据转换中最基础的几个功能，其他更详细的功能都可以在Design tab中找到 总的来说 kettle的操作逻辑就是 steps + hops，step是你的操作步骤，hops就是链接。</p><h2 id="作业-job">作业 JOB</h2><p>上面只介绍了kettle的核心功能之一转换，现在要介绍kettle的另一个功能job。</p><p>作业其实就是一个自动化流程，当你需要进行多个转换或者要增加一些逻辑控制条件的情况下，就需要job来实现。</p><p>这里就根据前面hello world的例子，实现一个简单的job，检测当文件夹内不存在file的时候，就自动从数据库中导出数据并添加hello。</p><p><img src="/images/kettle/kettle-13.png"></p><p>在右侧工具栏找到上图的控件拖进工作区，然后做hop连接，设置一下就行了。操作上和创建转换时类似的，只是将多个创建组合了起来</p><p>如下图是配置转换的界面，输入转换的脚本地址</p><p><img src="/images/kettle/kettle-14.png"></p><h2 id="厨房与煎锅">厨房与煎锅</h2><p>kitchen和pan指令是用于执行作业和转换脚本的命令。</p><figure class="highlight zsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./kitch.sh -file ./scripts/demo.kjb</span><br><span class="line">./pan.sh -file ./scripts/demo.ktr</span><br></pre></td></tr></table></figure><p>以后用spoon设计好的作业和转换只要保存下来，用命令行直接执行就好了，方便许多。</p><p>后期有用到新的功能就在更新笔记。</p><h2 id="参考资料">参考资料</h2><p><span class="exturl" data-url="aHR0cDovL3d3dy5jbmJsb2dzLmNvbS9saW1lbmdxaWFuZy9hcmNoaXZlLzIwMTMvMDEvMTYvS2V0dGxlQXBwbHkxLmh0bWwjc3l6ag==" title="http://www.cnblogs.com/limengqiang/archive/2013/01/16/KettleApply1.html#syzj">ETL利器Kettle实战应用解析系列一【Kettle使用介绍】<i class="fa fa-external-link"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTI2MzczNTgvYXJ0aWNsZS9kZXRhaWxzLzgyNTkzNDky" title="https://blog.csdn.net/u012637358/article/details/82593492">KETTLE使用教程<i class="fa fa-external-link"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25ld2Vhc3RzdW4vYXJ0aWNsZS9kZXRhaWxzLzM4ODQ1Nzk1" title="https://blog.csdn.net/neweastsun/article/details/38845795">kettle JOB使用<i class="fa fa-external-link"></i></span></p><p><span class="exturl" data-url="aHR0cHM6Ly9hc2suaGVsbG9iaS5jb20vYmxvZy95dWd1aXlhbmcxOTkwL2NhdGVnb3J5LzE1MzI=" title="https://ask.hellobi.com/blog/yuguiyang1990/category/1532">kettle教程<i class="fa fa-external-link"></i></span></p><p><span class="exturl" data-url="aHR0cDovL3d3dy5rZXR0bGUubmV0LmNuLw==" title="http://www.kettle.net.cn/">Kettle中文网<i class="fa fa-external-link"></i></span></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;kettle-入门教程&quot;&gt;kettle 入门教程&lt;/h1&gt;
&lt;p&gt;由于最后一个学期还有一个学分不够，就得再休一门课，看了看课表也就只能挑个OLAP了，又是一门完全陌生的课，然后网上关于kettle的教程又非常少，自己就搜罗了一些资料记录一下该课需要用的工具的笔记吧&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
      <category term="kettle" scheme="https://ereebay.me/tags/kettle/"/>
    
  </entry>
  
  <entry>
    <title>One-shot Learning with Memory-Augmented Neural Networks 论文笔记 【未完】</title>
    <link href="https://ereebay.me/posts/55756/"/>
    <id>https://ereebay.me/posts/55756/</id>
    <published>2019-03-22T00:47:11.000Z</published>
    <updated>2020-03-03T14:11:30.007Z</updated>
    
    <content type="html"><![CDATA[<h1 id="one-shot-learning-with-memory-augmented-neural-networks-论文笔记">One-shot Learning with Memory-Augmented Neural Networks 论文笔记</h1><h2 id="介绍">介绍</h2><p>基于梯度的传统的深度学习方法需要大量的数据进行学习，当遇到新的数据，模型就不得不重新学习新的参数，而无法快速适应新的数据。</p><p>本文提出了一种记忆增强神经网络能够快速的同化新的数据，并且在学习少两样本后利用该数据做出准确的预测。</p><a id="more"></a><p>对于MANN模型的设计，作者有两个条件：</p><ol type="1"><li>存储的信息必须稳定并且是元素级的寻址。</li><li>参数的数量不能束缚于存储空间的尺寸。</li></ol><p>最终该模型能够组合以下两种优势：</p><ol type="1"><li>能够基于梯度下降的方式获取原始数据中有用的信息来学习出一种通用的的学习方式。</li><li>能够基于额外的内存模块来快速的学习到从未见过信息。</li></ol><h2 id="元学习meta-learning方法设计">元学习（Meta-Learning）方法设计</h2><p>通常情况，都是通过在某个数据集<span class="math inline">\(D\)</span>上选择参数<span class="math inline">\(\theta\)</span>来最小化学习代价<span class="math inline">\(\mathcal{L}\)</span></p><p>但在元学习中，降低的是关于某个数据集分布<span class="math inline">\(p(D)\)</span>的期望代价:</p><p><span class="math display">\[\theta^{*}=\operatorname{argmin}_{\theta} E_{D \sim p(D)}[\mathcal{L}(D ; \theta)]\]</span></p><p>为了能够实现上述目的，本文做了如下的设置：</p><ol type="1"><li>首先，采用序列输入，每个输入包含着上一个输入所对应的标签</li></ol><p><span class="math display">\[\left(\mathbf{x}_{1}, \text { null }\right),\left(\mathbf{x}_{2}, y_{1}\right), \ldots,\left(\mathbf{x}_{T}, y_{T-1}\right)\]</span></p><ol start="2" type="1"><li>不同的数据集之间，标记会被打乱（防止网络逐渐学习样本和标签的映射关系），相反这使得网络学到在存储空间中保留数据样本直到下次正确的样本到来的技巧，这样就能够使得样本-标签信息绑定并且存储用于以后的使用。</li></ol><p><img src="../images/mann/mann-1.png"></p><p>元学习的模型会在不考虑数据及标签的实际内容前提下去学习绑定数据分布和对应的标签，并且会将泛化出一个通用的模型来映射数据与标签的关系用以预测。</p><h2 id="记忆增强模型">记忆增强模型</h2><h3 id="神经图灵机neural-turing-machine">神经图灵机（Neural Turing Machine）</h3><p>神经图灵机的组成和本文的MANN大致类似。神经图灵机是由LSTM或者前馈网络这样的控制器组成，利用大量的读写heads和外置的内存模块交互。</p><p>在该模型中，控制器同样采用LSTM或者前馈网络。</p><p>To be continued</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;one-shot-learning-with-memory-augmented-neural-networks-论文笔记&quot;&gt;One-shot Learning with Memory-Augmented Neural Networks 论文笔记&lt;/h1&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;基于梯度的传统的深度学习方法需要大量的数据进行学习，当遇到新的数据，模型就不得不重新学习新的参数，而无法快速适应新的数据。&lt;/p&gt;
&lt;p&gt;本文提出了一种记忆增强神经网络能够快速的同化新的数据，并且在学习少两样本后利用该数据做出准确的预测。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
  </entry>
  
  <entry>
    <title>生成对抗网络（GAN）相关笔记</title>
    <link href="https://ereebay.me/posts/59881/"/>
    <id>https://ereebay.me/posts/59881/</id>
    <published>2019-03-01T09:00:22.000Z</published>
    <updated>2019-03-03T12:28:01.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="generative-adversarial-network">Generative Adversarial Network</h1><h2 id="gan的概述">GAN的概述</h2><p>GAN的思想就是：这是一个两人的零和博弈游戏，博弈双方的利益之和是一个常数，比如两个人掰手腕，假设总的空间是一定的，你的力气大一点，那你就得到的空间多一点，相应的我的空间就少一点，相反我力气大我就得到的多一点，但有一点是确定的就是，我两的总空间是一定的，这就是二人博弈，但是呢总利益是一定的。</p><a id="more"></a><p>用一个形象的例子解释就是：GAN就好比是一个大的网络，在这个网络中有两个小的网络，一个是生成网络，可以当做是制作假钞的人， 而另一个是鉴别网络，也就是鉴别假钞的人。对于生成网络的目标就是去欺骗鉴别器，而鉴别器是为了不被生成器所欺骗。模型经过交替的优化训练，都能得到提升，理论证明，最后生成模型最好的效果是能够让鉴别器真假难分，也就是真假概率五五开。</p><!-- ![](../res/img/img1.jpg) --><p>上图是生成对抗网络的结构示意图，鉴别器接受真实样本和生成器生成的虚假样本，然后判断出真假结果。生成器接受噪声，生成出虚假样本。</p><h2 id="gan的原理">GAN的原理</h2><p>下式是GAN的目标函数公式：</p><p><span class="math display">\[\min _ { G } \max _ { D } V ( D , G ) = \mathbb { E } _ { \boldsymbol { x } \sim p _ { \mathrm { data } } ( \boldsymbol { x } ) } [ \log D ( \boldsymbol { x } ) ] + \mathbb { E } _ { \boldsymbol { z } \sim p _ { \boldsymbol { z } } ( z ) } [ \log ( 1 - D ( G ( \boldsymbol { z } ) ) ) ]\]</span></p><p>从目标函数可以看出，整个代价函数是最小化生成器，最大化鉴别器，那么在处理这个最优化问题的时候，我们可以先固定住G，然后先最大化D，然后再最小化G得到最优解。其中，在给定G的时候，最大化 V(D,G) 评估了P_G和P_data之间的差异或距离。</p><p>首先，在固定G之后，最优化D的情况就可以表述为：</p><p><span class="math display">\[D _ { G } ^ { * } = \operatorname { argmax } _ { D } V ( G , D )\]</span></p><p>最优化G的问题就可以表述为：</p><p><span class="math display">\[G ^ { * } = \operatorname { argmin } _ { G } V \left( G , D _ { G } ^ { * } \right)\]</span></p><h2 id="理论推导">理论推导</h2><p>在论文原文推导过程中采用的是JS散度来描述两个分布的相似程度，而JS散度其实使用KL散度构建出来的，因此，在进行完整推导之前，先介绍一些理论基础，在进行推导最优鉴别器和最优生成器所需要的条件，最后利用推导的结果重述训练过程。</p><h3 id="kl散度">KL散度</h3><p>对与同一个随机变量 <span class="math inline">\(x\)</span> 有两个单独的概率分布 <span class="math inline">\(P(x)\)</span> 和 <span class="math inline">\(Q(x)\)</span> ，用KL散度可以衡量这两个分布的差异（附录会有证明为何KL散度能说明两个分布的差异）：</p><p><span class="math display">\[D _ { \mathrm { KL } } ( P \| Q ) = \mathbb { E } _ { \mathrm { x } \sim P } \left[ \log \frac { P ( x ) } { Q ( x ) } \right] = \mathbb { E } _ { \mathrm { x } \sim P } [ \log P ( x ) - \log Q ( x ) ]\]</span></p><p>KL散度的性质：</p><ol type="1"><li>非负性（这个性质后面推导会用到），同时当且仅当P和Q相同分布时候，KL散度才会为0。因为这个非负性，所以它可以经常用来衡量两个分布之间的差异。（附录会证明其非负性）</li><li>非对称性， 虽然可以衡量分布的差异，但是这个差异的距离不是对称的，所以KL散度在P对于Q下的和Q对于P下两种情况是不同的。</li></ol><h3 id="论文推导过程中的问题">论文推导过程中的问题</h3><p>在原论文中，有一个思想和许多方法都不同，就是生成器G不需要满足可逆条件，在实践中，G确实就是不可逆的。但是在证明的过程中，大家都错误的使用了积分换元公式，而这个积分换元是只有当G满足可逆条件时候才能使用。所以证明应该是基于下面这个等式的成立性：</p><p><span class="math display">\[E _ { z \sim p _ { x } ( z ) } \log ( 1 - D ( G ( z ) ) ) = E _ { x \sim p _ { c } ( x ) } \log ( 1 - D ( x ) )\]</span></p><p>该等式来源于测度论重的Radon-Nikodym定理，它在原论文中的命题1中被展示，并表述为下列等式：</p><p><span class="math display">\[\begin{array} { c } { \int _ { x } p _ { d a t a } ( x ) \log D ( x ) \mathrm { d } x + \int _ { z } p ( z ) \log ( 1 - D ( G ( z ) ) ) \mathrm { d } z } \\ { = \int _ { x } p _ { d a t a } ( x ) \log D ( x ) + p _ { G } ( x ) \log ( 1 - D ( x ) ) \mathrm { d } x } \end{array}\]</span></p><p>这个公式中使用了积分换元公式，但是使用换元就必须计算G的逆，而G的逆没有被假定存在。而且在神经网络中的实践中，它也不存在。不过这方法在ML中太常见了，因此就忽略了。</p><h3 id="最优判别器">最优判别器</h3><p>在极小极大博弈中，首先固定生成器G，最大化价值函数，从而得出最优判别起D。其中，最大化的价值函数评估了生成器生成的数据分布和数据集分布的差异（后面会证明）。</p><p>在原论文中的价值函数可以将其数学期望展开成为积分的形式：</p><p><span class="math display">\[V ( G , D ) = \int _ { x } p _ { \mathrm { data } } ( \boldsymbol { x } ) \log ( D ( \boldsymbol { x } ) ) + p _ { g } ( \boldsymbol { x } ) \log ( 1 - D ( \boldsymbol { x } ) ) d x\]</span></p><p>通过求积分的最大值可以转化为求被积函数的最大值。通过求被积函数的最大值可以求的最优判别器D，因此可以把不涉及到鉴别器的项都看作是常数项，令鉴别器D(x)为y，则被积函数可以表示为：</p><p><span class="math display">\[f ( y ) = a \log y + b \log ( 1 - y )\]</span></p><p>为了寻找最优的极值点，如果<span class="math inline">\(a + b \neq 0\)</span>，可以用来进行下一阶导求解：</p><p><span class="math display">\[f ^ { \prime } ( y ) = 0 \Rightarrow \frac { a } { y } - \frac { b } { 1 - y } = 0 \Rightarrow y = \frac { a } { a + b }\]</span></p><p>通过在其驻点进行二阶导求解可得：</p><p><span class="math display">\[f ^ { \prime \prime } \left( \frac { a } { a + b } \right) = - \frac { a } { \left( \frac { a } { a + b } \right) ^ { 2 } } - \frac { b } { 1 - \left( \frac { a } { a + b } \right) ^ { 2 } } &lt; 0\]</span></p><p>其中<span class="math inline">\(a , b \in ( 0,1 )\)</span>。因为一阶导等于0，二阶导小于0，因此<span class="math inline">\(\frac { a } { a + b }\)</span>时就是极大值。</p><p>最后就可以将价值函数写为：</p><p><span class="math display">\[\begin{aligned} V ( G , D ) = &amp; \int _ { x } p _ { d a t a } ( x ) \log D ( x ) + p _ { G } ( x ) \log ( 1 - D ( x ) ) \mathrm { d } x \\ &amp; \leq \int \max _ { y } \max _ { y } p _ { d a t a } ( x ) \log y + p _ { G } ( x ) \log ( 1 - y ) \mathrm { d } x \end{aligned}\]</span></p><p>令D(x)=P_data/(P_data+p_G)，就可以取得极大值，因为在丁一宇内f(y)拥有唯一的极大值，也就是说最优D是唯一的，并且没有其他的D能实现极大值。</p><p>事实上，最优D在实践中是无法被计算，但是在数学上很重要。此外，我们不知道先验的Pdata，所以我们不能直接在训练中使用它。另一方面来说，最优D的存在证明了最优G，而且我们只要趋向于最优的D就可以了。</p><h3 id="最优生成器">最优生成器</h3><p>GAN的训练过程就是为了让P_G=P_data，那么最优D就可以表示为：</p><p><span class="math display">\[D _ { G } ^ { * } = \frac { p _ { \text {data} } } { p _ { \text {data} } + p _ { G } } = \frac { 1 } { 2 }\]</span></p><p>也就是最优的生成器会使得鉴别器无法辨别P_data和P_G。基于这个观点，论文作者证明了G就是极大极小博弈的解。</p><p>定理：当且仅当P_G=P_data的时候，C(G)=maxV(G,D)的全局最小点可以达到。</p><p>定理中说该结论是当且仅当是成立，因此从两个方向证明。首先反向逼近，证明C(G)的取值，然后从正向证明。</p><p>假设P_G=P_data(反向预先知道结果进行推到)，可以反向推出：</p><p><span class="math display">\[V \left( G , D _ { G } ^ { * } \right) = \int _ { x } p _ { d a t a } ( x ) \log \frac { 1 } { 2 } + p _ { G } ( x ) \log \left( 1 - \frac { 1 } { 2 } \right) \mathrm { d } x\]</span></p><p><span class="math display">\[V \left( G , D _ { G } ^ { * } \right) = - \log 2 \int _ { x } p _ { G } ( x ) \mathrm { d } x - \log 2 \int _ { x } p _ { d a t a } ( x ) \mathrm { d } x = - 2 \log 2 = - \log 4\]</span></p><p>那么-log4就是最小值的候选，因为只有在P_G=P_data的时候才出现。现在就要从正向证明这个值常常是最小值，也就是同时满足当且仅当的条件。</p><p>现在放弃P_G=P_data的条件，选取任何的G，改写公式为：</p><p><span class="math display">\[C ( G ) = \int _ { x } p _ { d a t a } ( x ) \log \left( \frac { p _ { d a t a } ( x ) } { p _ { G } ( x ) + p _ { d a t a } ( x ) } \right) + p _ { G } ( x ) \log \left( \frac { p _ { G } ( x ) } { p _ { G } ( x ) + p _ { d a t a } ( x ) } \right) \mathrm { d } x\]</span></p><p>下面会应用一个trick，给方程增加一个0，不改变方程的值，但是可以构建出一个log2，因为我们知道-log4是全局最小值的候选。</p><p><span class="math display">\[\begin{aligned} C ( G ) &amp; = \int _ { x } ( \log 2 - \log 2 ) p _ { d a t a } ( x ) + p _ { d a t a } ( x ) \log \left( \frac { p _ { d a t a } ( x ) } { p _ { G } ( x ) + p _ { d a t a } ( x ) } \right) \\ &amp; + ( \log 2 - \log 2 ) p _ { G } ( x ) + p _ { G } ( x ) \log \left( \frac { p _ { G } ( x ) } { p _ { G } ( x ) + p _ { d a t a } ( x ) } \right) \mathrm { d } x \end{aligned}\]</span></p><p><span class="math display">\[\begin{array} { c } { C ( G ) = - \log 2 \int _ { x } p _ { G } ( x ) + p _ { d a t a } ( x ) d x } \\ { + \int _ { x } p _ { d a t a } ( x ) \left( \log 2 + \log \left( \frac { p _ { d a t a } ( x ) } { p _ { G } ( x ) + p _ { d a t a } ( x ) } \right) \right) } \\ { + p _ { G } ( x ) \left( \log 2 + \log \left( \frac { p _ { G } ( x ) } { p _ { G } ( x ) + p _ { d a t a } ( x ) } \right) \right) \mathrm { d } x } \end{array}\]</span></p><p>最后化简可以得到：</p><p><span class="math display">\[\begin{aligned} C ( G ) = &amp; - \log 4 + \int _ { x } p _ { d a t a } ( x ) \log \left( \frac { p _ { d a t a } ( x ) } { \left( p _ { G } ( x ) + p _ { \text {data} } ( x ) \right) / 2 } \right) \mathrm { d } x \\ &amp; + \int _ { x } p _ { G } ( x ) \log \left( \frac { p _ { G } ( x ) } { \left( p _ { G } ( x ) + p _ { d a t a } ( x ) \right) / 2 } \right) \mathrm { d } x \end{aligned}\]</span></p><p>如果阅读了前面kl散度的部分，就可以发现他可以化简为kl散度的形式：</p><p><span class="math display">\[C ( G ) = - \log 4 + K L \left( p _ { d a t a } | \frac { p _ { d a t a } + p _ { G } } { 2 } \right) + K L \left( p _ { G } | \frac { p _ { d a t a } + p _ { G } } { 2 } \right)\]</span></p><p>由于KL散度是非负的，所以-log4就是全局最小值。</p><p>接下来只需要证明只有这一个G能达到这个值，这样P_G=P_data就成为了唯一解，整个证明就完成了。</p><p>从前文可以KL散度是非对称，只能衡量分布a对于分布b的相似度，但是加上了后面那项以后，他们的和就是对称得了，这个时候，这两项的和就能用JS散度来表示：</p><p><span class="math display">\[\operatorname { JSD } ( P \| Q ) = \frac { 1 } { 2 } D ( P \| M ) + \frac { 1 } { 2 } D ( Q \| M )\]</span></p><p><span class="math display">\[M = \frac { 1 } { 2 } ( P + Q )\]</span></p><p>假设存在两个分布P和Q，且这两个分布的平均分布M=(P+Q)/2，那么这两个分布之间的JS散度就是P和M之间的KL散度加上Q和M之间的KL散度除以2。</p><p>因此JS的散度取值范围是0到log2。当两个分布完全不存在交集则为log2， 当完全一样的时候则是最小值0.</p><p>因此C(G)可以改写为：</p><p><span class="math display">\[C ( G ) = - \log 4 + 2 \cdot J S D \left( p _ { \text { data } } | p _ { G } \right)\]</span></p><p>也就证明了，当P_G=P_data时，JSD为0。综上，当生成的分布当且仅当等于真实数据分布的时候，我们取得了最优的生成器。</p><h3 id="收敛性">收敛性</h3><p>关于训练过程中能否收敛到最优的生成器，原论文有额外的证明表示，当具有足够的训练数据，d和g有足够的性能时候，是可以收敛到最优G，因为这块内容不是特别重要，证明我会放在附录（主要我也看的一知半解）</p><h3 id="训练过程">训练过程</h3><p>1.参数优化过程</p><p>若我们要寻找最优的生成器，在确定一个鉴别器D以后，我们可以把原本的价值函数看作是训练生成器的损失函数L(G)。有了损失函数，就可以通过SGD，Adam等优化算法更新我们的生成器，梯度下降的优化过程如下：</p><p><span class="math display">\[\theta _ { G } \leftarrow \theta _ { G } - \eta \partial L ( G ) / \partial \theta _ { G }\]</span></p><p>现在给定一个初始的G_0，需要找到令V(G_0,D)最大的D_0*，因此鉴别器的更新过程就是损失函数：-V（G，D）的过程。并且有前面的推导可知，V(G,D)实际上与分布P_data(x)和P_G(x)之间的JS散度只相差了一个常数项，因此这样的循环对抗过程能表述为：</p><ul><li>给定G_0，最大化V(G_0,D)以求得D_0*，即max[JSD(P_data(x)||P_G0(x))];</li><li>固定D_0*，计算<span class="math inline">\(\mathrm { \theta } _ { - } \mathrm { G } 1 \leftarrow \theta _ { - } \mathrm { G0 } - \mathrm { \eta } \left( \partial \mathrm { V } \left( \mathrm { G } , \mathrm { D } _ { - } \mathrm { 0 } ^ { * } \right) / \partial \theta _ { - } \mathrm { G } \right)\)</span>，求得更新后的G_1;</li><li>固定G_1，最大化V(G_1,D_0<em>)以求得D_1</em>，即max[JSD(P_data(x)||P_G1(x)];</li><li>固定D_1*，计算<span class="math inline">\(\theta _ { - } \mathrm { G } 2 \leftarrow \theta _ { - } \mathrm { G } 1 - \eta \left( \partial \mathrm { V } \left( G , D _ { - } \mathrm { 0 } ^ { * } \right) / \partial \theta _ { - } \mathrm { G } \right)\)</span>以求得更新后的G_2;</li></ul><p>如此循环</p><p>2.实际训练过程</p><p>根据前面价值函数V(G,D)的定义，我们需要求两个数学期望，即E[log(D(x))]和E[log(1-D(G(z)))]，其中x服从真实数据分布，z服从初始化分布。但在实践中，我们是没用办法利用积分求这两个数学期望，所以一般只能通过从无穷的真实数据和无穷的生成器中采样来逼近数学期望。</p><p>假设现在给定生成器G，并希望计算maxV(G,D)来求鉴别器D，那么首先我们需要从P_data(x)中采样m个样本，从生成器P_G(x)采样m哥样本。因此最大化价值函数可以用下面表达式代替：</p><p><span class="math display">\[\text { Maximize } \tilde { V } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log D \left( x ^ { i } \right) + \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log \left( 1 - D \left( \tilde { x } ^ { i } \right) \right)\]</span></p><p>现在我们将从P_data(x)中抽取的样本作为证样本，从P_G(x)抽取的样本作为负样本，同时逼近负V(G,D)的函数作为损失函数，因此就可以将其表述为一个标准的二分类器的训练过程：</p><p><span class="math display">\[\text { Minimize } L = - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log D \left( x ^ { i } \right) - \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log \left( 1 - D \left( \tilde { x } ^ { i } \right) \right)\]</span></p><p>在实践中，我们必须使用迭代和数值计算的方法来实现极大极小化的博弈过程。在训练的内部循环中完整优化D在计算上是不允许的，而且有限的数据集会导致过拟合。因此我们可以在k个优化D步骤和一个优化G的步骤交替进行。只要慢慢更新G，D就会一直处于最优解的附近。</p><p>综上，在整个训练过程中，对于每一次迭代：</p><ul><li>从真实数据分布P_data中抽取m个样本</li><li>从先验分布P_prior(z)抽取m个噪声样本</li><li>将噪声样本输入到生成器G中生成数据<span class="math inline">\(\left\{ \tilde { x } ^ { 1 } , \tilde { x } ^ { 2 } , \ldots , \tilde { x } ^ { m } \right\} , \tilde { x } ^ { i } = G \left( z ^ { i } \right)\)</span>，通过最大化V的近似而更新鉴别器参数<span class="math inline">\(\theta _ { - } d\)</span>，鉴别器参数的更新迭代公式为<span class="math inline">\(\theta _ { d } \leftarrow \theta _ { d } + \eta \nabla \tilde { V } \left( \theta _ { d } \right)\)</span></li></ul><p>以上是学习鉴别器D的过程。因为学习的过程是计算JS散度的过程，并且会重复k次，因为我们希望能够最大化价值函数。</p><ul><li>从先验分布P_prior(z)中抽取另外m个噪声样本</li><li>通过极小化V，也就是<span class="math inline">\(\tilde { V } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \log \left( 1 - D \left( G \left( z ^ { i } \right) \right) \right)\)</span>，且生成器参数的更新迭代式为<span class="math inline">\(\theta _ { g } \leftarrow \theta _ { g } - \eta \nabla \tilde { V } \left( \theta _ { g } \right)\)</span></li></ul><p>以上是生成器参数的学习过程，这个过程只会在一次迭代中出现一次，因此能避免更新太多使得js散度上升。</p><p>以上便是GAN的完整推导过程和论证。</p><h2 id="gan训练的几个问题">GAN训练的几个问题</h2><h3 id="训练不稳定">训练不稳定</h3><p>原始的GAN训练非常困难。主要体现在训练过程中可能并不收训练出得生成器根本不能产生有意义的内容等方面。另一方面，虽然说我们的优化目标函数是js散度，他应该能体现处两个分布的距离。并且这个距离一开始最好比较大，最后随着训练G过程的深入，这个距离应该慢慢变小才比较好。</p><p>在实际过程中，鉴别器的损失函数非常容易变成0，而且在后面的过程中也一直保持0。js散度是用来衡量两个分布之间的距离，但实际上有两种情况会导致js散度判定两个分布距离是无穷大，从而使得lossfunction永远是0。</p><p>情况1: 鉴别器D过强，导致了过拟合。</p><p>解决方法：尝试使用正则化，或者减少模型参数</p><p>情况2: 数据本身的特性。生成器产生的低维流型确实不容易产生重叠。</p><p>解决方法：一种是给数据添加噪声，让生成器和真实数据分布更容易重叠 还有一种方法是下次要将的GAN</p><h3 id="模式崩溃-mode-collapse">模式崩溃 mode collapse</h3><p>所有的输出都一样！这个现象被称为Mode Collapse。这个现象产生的原因可能是由于真实数据在空间中很多地方都有一个较大的概率值，但是我们的生成模型没有直接学习到真实分布的特性。为了保证最小化损失，它会宁可永远输出一样但是肯定正确的输出，也不愿意尝试其他不同但可能错误的输出。也就是说，我们的生成器有时可能无法兼顾数据分布的所有内部模式，只会保守地挑选出一个肯定正确的模式。</p><h2 id="总结">总结</h2><ul><li>GAN结合了生成模型和鉴别模型，消除了生成模型的损失函数难以定义的问题</li><li>基于概率分布来计算，不受生成维度的限制</li><li>可以用来进行半监督学习</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;generative-adversarial-network&quot;&gt;Generative Adversarial Network&lt;/h1&gt;
&lt;h2 id=&quot;gan的概述&quot;&gt;GAN的概述&lt;/h2&gt;
&lt;p&gt;GAN的思想就是：这是一个两人的零和博弈游戏，博弈双方的利益之和是一个常数，比如两个人掰手腕，假设总的空间是一定的，你的力气大一点，那你就得到的空间多一点，相应的我的空间就少一点，相反我力气大我就得到的多一点，但有一点是确定的就是，我两的总空间是一定的，这就是二人博弈，但是呢总利益是一定的。&lt;/p&gt;
    
    </summary>
    
      <category term="notes" scheme="https://ereebay.me/categories/notes/"/>
    
    
      <category term="GAN" scheme="https://ereebay.me/tags/GAN/"/>
    
      <category term="note" scheme="https://ereebay.me/tags/note/"/>
    
      <category term="deep learning" scheme="https://ereebay.me/tags/deep-learning/"/>
    
  </entry>
  
</feed>
